{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f49737f",
   "metadata": {},
   "source": [
    "# Class Pratical Assigment\n",
    "#### 21.03.2022\n",
    "\n",
    "\n",
    "\n",
    "### Data Set Information:\n",
    "\n",
    "This data set was generated to model psychological experimental results. Each example is classified as having the balance scale tip to the right, tip to the left, or be balanced. The attributes are the left weight, the left distance, the right weight, and the right distance. The correct way to find the class is the greater of (left-distance * left-weight) and (right-distance * right-weight). If they are equal, it is balanced.\n",
    "\n",
    "\n",
    "### Attribute Information:\n",
    "\n",
    "1. Class Name: 3 (L, B, R)\n",
    "2. Left-Weight: 5 (1, 2, 3, 4, 5)\n",
    "3. Left-Distance: 5 (1, 2, 3, 4, 5)\n",
    "4. Right-Weight: 5 (1, 2, 3, 4, 5)\n",
    "5. Right-Distance: 5 (1, 2, 3, 4, 5)\n",
    "\n",
    "### References: \n",
    " - https://archive.ics.uci.edu/ml/datasets/Balance+Scale\n",
    " - https://github.com/TheAIFramework/PracticalMachineLearning/blob/Trees/4%20-%20Ensemble%20Methods.ipynb\n",
    " - https://github.com/TheAIFramework/PracticalMachineLearning/blob/Trees/3%20-%20Decision%20Trees.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "4c1a893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "a907c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('balance-scale.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "382adcdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>1.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>B</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>624 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     B  1  1.1  1.2  1.3\n",
       "0    R  1    1    1    2\n",
       "1    R  1    1    1    3\n",
       "2    R  1    1    1    4\n",
       "3    R  1    1    1    5\n",
       "4    R  1    1    2    1\n",
       "..  .. ..  ...  ...  ...\n",
       "619  L  5    5    5    1\n",
       "620  L  5    5    5    2\n",
       "621  L  5    5    5    3\n",
       "622  L  5    5    5    4\n",
       "623  B  5    5    5    5\n",
       "\n",
       "[624 rows x 5 columns]"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "210d63c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 624 entries, 0 to 623\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   B       624 non-null    object\n",
      " 1   1       624 non-null    int64 \n",
      " 2   1.1     624 non-null    int64 \n",
      " 3   1.2     624 non-null    int64 \n",
      " 4   1.3     624 non-null    int64 \n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 24.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "5775886d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>1.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>624.000000</td>\n",
       "      <td>624.000000</td>\n",
       "      <td>624.000000</td>\n",
       "      <td>624.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.003205</td>\n",
       "      <td>3.003205</td>\n",
       "      <td>3.003205</td>\n",
       "      <td>3.003205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.414210</td>\n",
       "      <td>1.414210</td>\n",
       "      <td>1.414210</td>\n",
       "      <td>1.414210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                1         1.1         1.2         1.3\n",
       "count  624.000000  624.000000  624.000000  624.000000\n",
       "mean     3.003205    3.003205    3.003205    3.003205\n",
       "std      1.414210    1.414210    1.414210    1.414210\n",
       "min      1.000000    1.000000    1.000000    1.000000\n",
       "25%      2.000000    2.000000    2.000000    2.000000\n",
       "50%      3.000000    3.000000    3.000000    3.000000\n",
       "75%      4.000000    4.000000    4.000000    4.000000\n",
       "max      5.000000    5.000000    5.000000    5.000000"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "ebf6bca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B      0\n",
       "1      0\n",
       "1.1    0\n",
       "1.2    0\n",
       "1.3    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check null values existence\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "8200dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "81739111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      R\n",
       "1      R\n",
       "2      R\n",
       "3      R\n",
       "4      R\n",
       "      ..\n",
       "619    L\n",
       "620    L\n",
       "621    L\n",
       "622    L\n",
       "623    B\n",
       "Name: B, Length: 624, dtype: object"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TARGET ARRAY\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "cc844509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R    288\n",
       "L    288\n",
       "B     48\n",
       "Name: B, dtype: int64"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "ac85dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "# drop one column by name\n",
    "X.drop('B', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "5712c6a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>1.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>624 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  1.1  1.2  1.3\n",
       "0    1    1    1    2\n",
       "1    1    1    1    3\n",
       "2    1    1    1    4\n",
       "3    1    1    1    5\n",
       "4    1    1    2    1\n",
       "..  ..  ...  ...  ...\n",
       "619  5    5    5    1\n",
       "620  5    5    5    2\n",
       "621  5    5    5    3\n",
       "622  5    5    5    4\n",
       "623  5    5    5    5\n",
       "\n",
       "[624 rows x 4 columns]"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FEATURE MATRIX\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "1c0c790a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>1.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003215</td>\n",
       "      <td>-0.003215</td>\n",
       "      <td>-0.003215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.1</th>\n",
       "      <td>-0.003215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003215</td>\n",
       "      <td>-0.003215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2</th>\n",
       "      <td>-0.003215</td>\n",
       "      <td>-0.003215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.3</th>\n",
       "      <td>-0.003215</td>\n",
       "      <td>-0.003215</td>\n",
       "      <td>-0.003215</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1       1.1       1.2       1.3\n",
       "1    1.000000 -0.003215 -0.003215 -0.003215\n",
       "1.1 -0.003215  1.000000 -0.003215 -0.003215\n",
       "1.2 -0.003215 -0.003215  1.000000 -0.003215\n",
       "1.3 -0.003215 -0.003215 -0.003215  1.000000"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "264c9346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ManuelSousa\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ManuelSousa\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ManuelSousa\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ManuelSousa\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAEGCAYAAABbzE8LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAJ5UlEQVR4nO3dW4icdxnH8d/TpGDqAZGUWrZqkL0QFQ8llErFEyKe0Au9KHgAEUSRJeKFoBeKeC/WvZGiouIJwQNSqiioeCEqSa1aaSmDtthY22qxrSQqrX8vdgpxu0lmk9n32U0+Hwid3Xk3/4c/73yZfaczqTFGAJjeJd0DAFysBBigiQADNBFggCYCDNBk/3YOPnjw4Dh06NAOjQJwYTp27NjfxhiXb/7+tgJ86NChHD16dHlTAVwEqururb7vEgRAEwEGaCLAAE0EGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNtvVvwjGN9fX1zGaz7jH2jOPHjydJVlZWmifZO1ZXV7O2ttY9xkVPgHeh2WyWW2+7PY9d9ozuUfaEfSceSpL89d9O50XsO/Fg9wjMOWN3qccue0ZOPu+N3WPsCQfuuDlJ7NeCHt8v+rkGDNBEgAGaCDBAEwEGaCLAAE0EGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNBBigiQADNBFggCYCDNBEgAGaCDBAEwEGaCLAAE0EGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAmAgzQZJIAr6+vZ319fYqlAJZqJ/u1f0f+1k1ms9kUywAs3U72yyUIgCYCDNBEgAGaCDBAEwEGaCLAAE0EGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNBBigiQADNBFggCYCDNBEgAGaCDBAEwEGaCLAAE0EGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAm+6dY5Pjx4zl58mSOHDkyxXJ73mw2yyX/Gd1jcIG65F8PZzZ7xONxQbPZLAcOHNiRv/usz4Cr6n1VdbSqjj7wwAM7MgTAxeisz4DHGDcmuTFJDh8+fE5Py1ZWVpIkN9xww7n8+EXnyJEjOfbH+7rH4AL13yc9LavPvcLjcUE7+ZuCa8AATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNBBigiQADNBFggCYCDNBEgAGaCDBAEwEGaCLAAE0EGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNBBigiQADNBFggCYCDNBEgAGaCDBAEwEGaCLAAE0EGKDJ/ikWWV1dnWIZgKXbyX5NEuC1tbUplgFYup3sl0sQAE0EGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNBBigiQADNBFggCYCDNBEgAGaCDBAEwEGaCLAAE0EGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmiyv3sAtrbvxIM5cMfN3WPsCftO/D1J7NeC9p14MMkV3WMQAd6VVldXu0fYU44ffzRJsrIiKou5wjm2SwjwLrS2ttY9AjAB14ABmggwQBMBBmgiwABNBBigiQADNBFggCYCDNBEgAGaCDBAEwEGaCLAAE0EGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATWqMsfjBVQ8kufsc1zqY5G/n+LM7yVzbY67tMdf2XKhzPWeMcfnmb24rwOejqo6OMQ5Pstg2mGt7zLU95tqei20ulyAAmggwQJMpA3zjhGtth7m2x1zbY67tuajmmuwaMAD/zyUIgCYCDNBkqQGuqi9W1f1Vddtp7q+q+mxVzarqd1V19TLXP4+5XlVVD1XVrfM/H59ormdV1U+r6vaq+kNVHdnimMn3bMG5Jt+zqnpSVf26qn47n+uTWxzTsV+LzNVyjs3X3ldVv6mqm7a4r+UxucBcXY/Ju6rq9/M1j25x/3L3a4yxtD9JXpHk6iS3neb+Nyb5QZJKcm2SXy1z/fOY61VJbppilk3rXpnk6vntpya5M8nzu/dswbkm37P5HjxlfvvSJL9Kcu0u2K9F5mo5x+ZrfzjJ17dav+sxucBcXY/Ju5IcPMP9S92vpT4DHmP8PMmDZzjkrUm+Mjb8MsnTq+rKZc5wjnO1GGPcO8a4ZX77kSS3J1nZdNjke7bgXJOb78E/519eOv+z+VXkjv1aZK4WVXVVkjcl+fxpDml5TC4w12611P2a+hrwSpI/n/L1PdkFD+y5l81/hfxBVb1g6sWr6lCSl2bj2dOpWvfsDHMlDXs2/7X11iT3J/nxGGNX7NcCcyU959hnknwkyX9Pc3/X+fWZnHmupGe/RpIfVdWxqnrfFvcvdb+mDnBt8b3d8Ezhlmy8V/vFSdaTfG/KxavqKUm+neRDY4yHN9+9xY9Msmdnmatlz8YYj40xXpLkqiTXVNULNx3Ssl8LzDX5flXVm5PcP8Y4dqbDtvjeju7XgnN1PSavG2NcneQNST5YVa/YdP9S92vqAN+T5FmnfH1Vkr9MPMMTjDEefvxXyDHGzUkuraqDU6xdVZdmI3JfG2N8Z4tDWvbsbHN17tl8zX8k+VmS12+6q/UcO91cTft1XZK3VNVdSb6Z5DVV9dVNx3Ts11nn6jq/xhh/mf/3/iTfTXLNpkOWul9TB/j7Sd49fyXx2iQPjTHunXiGJ6iqZ1ZVzW9fk419+fsE61aSLyS5fYzx6dMcNvmeLTJXx55V1eVV9fT57QNJXpvkjk2HdezXWefq2K8xxkfHGFeNMQ4luT7JT8YY79x02OT7tchcTefXk6vqqY/fTvK6JJv/z6ml7tf+c552C1X1jWy8enmwqu5J8olsvCCRMcbnktycjVcRZ0lOJHnPMtc/j7nenuQDVfVokpNJrh/zlzx32HVJ3pXk9/Prh0nysSTPPmW2jj1bZK6OPbsyyZeral82HpDfGmPcVFXvP2Wujv1aZK6uc+wJdsF+LTJXx35dkeS78+7vT/L1McYPd3K/vBUZoIl3wgE0EWCAJgIM0ESAAZoIMEATAWbPqLN/qt075p9Q9buq+kVVvXjqGWE7BJi95Et54jvfTvWnJK8cY7woyaeye/95G0iy5DdiwE4aY/x8/uFAp7v/F6d8+ctsvE0Udi3PgLlQvTcbn9sKu5ZnwFxwqurV2Qjwy7tngTMRYC4oVfWibHzI9xvGGDv+gUpwPlyC4IJRVc9O8p0k7xpj3Nk9D5yND+Nhzzj1U+2S3JdNn2pXVZ9P8rYkd89/5NExxuGGUWEhAgzQxCUIgCYCDNBEgAGaCDBAEwEGaCLAAE0EGKDJ/wAhy2aEVV35JwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAEGCAYAAABbzE8LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAJ70lEQVR4nO3dXYikZ5nH4f+dmYhjVEQmxNBxHdY+EBU/hhAiEQki4rqL7rKCHqigB0GRZsQDQQ8U8VwS+0SCCopfCJpliXFZQZc9WpcZjSaSIIVEzERNNJgYZ/xKHg+6AmOnZ6ZqUv3e3TPXBU2qu96e5+ah6kf1W91vaowRAKZ3WfcAAJcqAQZoIsAATQQYoIkAAzQ5uMzBhw8fHkeOHNmlUQAuTidOnPjNGOPK7V9fKsBHjhzJ8ePHVzcVwCWgqn6+09edggBoIsAATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNBBigiQADNBFggCYCDNBEgAGaCDBAEwEGaCLAAE0EGKDJUv9POKaxubmZ2WzWPca+cfLkySTJ2tpa8yT7x/r6ejY2NrrHuOQJ8B40m81y59335PFnPb97lH3hwKlHkiS/+pOH8yIOnHq4ewTmPGL3qMef9fycfsmbu8fYFw7de0eS2K8FPblf9HMOGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNBBigiQADNBFggCYCDNBEgAGaCDBAEwEGaCLAAE0EGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNBBigySQB3tzczObm5hRLAazUbvbr4K78q9vMZrMplgFYud3sl1MQAE0EGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNBBigiQADNBFggCYCDNBEgAGaCDBAEwEGaCLAAE0EGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNDk6xyMmTJ3P69OkcO3ZsiuX2vdlslsv+PLrH4CJ12R8fzWz2e8/HBc1msxw6dGhX/u3zvgKuqpuq6nhVHX/ooYd2ZQiAS9F5XwGPMW5NcmuSXHvttRf0smxtbS1Jcsstt1zIt19yjh07lhM/+3X3GFyknnjmc7P+j1d5Pi5oN39ScA4YoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNBBigiQADNBFggCYCDNBEgAGaCDBAEwEGaCLAAE0EGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNBBigiQADNBFggCYCDNBEgAGaCDBAEwEGaCLAAE0EGKCJAAM0OTjFIuvr61MsA7Byu9mvSQK8sbExxTIAK7eb/XIKAqCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNBBigiQADNBFggCYCDNBEgAGaCDBAEwEGaCLAAE0EGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNDnYPwM4OnHo4h+69o3uMfeHAqd8mif1a0IFTDye5qnsMIsB70vr6evcI+8rJk39NkqyticpirvIY2yMEeA/a2NjoHgGYgHPAAE0EGKCJAAM0EWCAJgIM0ESAAZoIMEATAQZoIsAATQQYoIkAAzQRYIAmAgzQRIABmggwQBMBBmgiwABNBBigiQADNBFggCY1xlj84KqHkvz8Atc6nOQ3F/i9u8lcyzHXcsy1nIt1rheNMa7c/sWlAvx0VNXxMca1kyy2BHMtx1zLMddyLrW5nIIAaCLAAE2mDPCtE661DHMtx1zLMddyLqm5JjsHDMDfcwoCoIkAAzRZaYCr6vNV9WBV3X2W+6uqPl1Vs6r6cVUdXeX6T2OuG6vqkaq6c/7xsYnmemFVfa+q7qmqn1TVsR2OmXzPFpxr8j2rqmdW1f9X1Y/mc31ih2M69muRuVoeY/O1D1TVD6vq9h3ua3lOLjBX13Pyvqq6a77m8R3uX+1+jTFW9pHkdUmOJrn7LPe/Ocm3k1SS65N8f5XrP425bkxy+xSzbFv36iRH57efk+SnSV7avWcLzjX5ns334Nnz25cn+X6S6/fAfi0yV8tjbL72h5J8Zaf1u56TC8zV9Zy8L8nhc9y/0v1a6SvgMcb/Jnn4HIe8NckXx5b/S/K8qrp6lTNc4Fwtxhi/HGP8YH7790nuSbK27bDJ92zBuSY334PH5p9ePv/Y/i5yx34tMleLqromyT8n+exZDml5Ti4w11610v2a+hzwWpJfnPH5/dkDT+y518x/hPx2Vb1s6sWr6kiSV2fr1dOZWvfsHHMlDXs2/7H1ziQPJvnOGGNP7NcCcyU9j7Gbk3w4yRNnub/r8XVzzj1X0rNfI8l/V9WJqrpph/tXul9TB7h2+NpeeKXwg2z9rfYrk2wm+Y8pF6+qZyf5RpIPjjEe3X73Dt8yyZ6dZ66WPRtjPD7GeFWSa5JcV1Uv33ZIy34tMNfk+1VV/5LkwTHGiXMdtsPXdnW/Fpyr6zl5wxjjaJJ/SvKBqnrdtvtXul9TB/j+JC884/Nrkjww8QxPMcZ49MkfIccYdyS5vKoOT7F2VV2erch9eYzxzR0Oadmz883VuWfzNX+X5H+SvGnbXa2PsbPN1bRfNyR5S1Xdl+RrSV5fVV/adkzHfp13rq7H1xjjgfl/H0xyW5Lrth2y0v2aOsD/meTd83cSr0/yyBjjlxPP8BRV9YKqqvnt67K1L7+dYN1K8rkk94wxPnWWwybfs0Xm6tizqrqyqp43v30oyRuS3LvtsI79Ou9cHfs1xvjIGOOaMcaRJO9I8t0xxju3HTb5fi0yV9Pj64qqes6Tt5O8Mcn235xa6X4dvOBpd1BVX83Wu5eHq+r+JB/P1hsSGWN8Jskd2XoXcZbkVJL3rHL9pzHX25K8v6r+muR0kneM+Vueu+yGJO9Kctf8/GGSfDTJP5wxW8eeLTJXx55dneQLVXUgW0/Ir48xbq+q950xV8d+LTJX12PsKfbAfi0yV8d+XZXktnn3Dyb5yhjjv3Zzv/wpMkATfwkH0ESAAZoIMEATAQZoIsAATQSYfaPOf1W7t86vUHVnVR2vqtdOPSMsw6+hsW/M/yz0sWxdDGX7n/o++afTfxhjjKp6RbZ+H/clU88Ji/IKmH3jfFe1G2M8dsYv61+RvXGdETgrAeaiUlX/VlX3JvlWkvd2zwPnIsBcVMYYt81PO/xrkk82jwPnJMBclOanK1485RXaYFkCzEWjqtbPuILW0STPyARXtYMLtdKrocFuWuCqdv+erUsF/iVbV9B6e9cVx2ARfg0NoIlTEABNBBigiQADNBFggCYCDNBEgAGaCDBAk78BGXx3f4s6wocAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for outliers in dataset\n",
    "X_columns = X.columns\n",
    "for col in X_columns:\n",
    "    fig = plt.figure()\n",
    "    sns.boxplot(X[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b67a7c",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    " - Neither null values or outliers were found in the feature dataset!\n",
    " - Not much to say about features correlation...\n",
    " - Target values seem very balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de414ff",
   "metadata": {},
   "source": [
    "## Data Training/Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "33aaa300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 training and testing splitting - Pareto principle\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "97003e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# series = []\n",
    "# for i in range(1, 5):\n",
    "#     split_size = round(0.1*i, 1)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_size)\n",
    "#     series.append([X_train, X_test, y_train, y_test])\n",
    "    \n",
    "# % de splitting\n",
    "# modelo | accuracy_treino | accuracy_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff49fad7",
   "metadata": {},
   "source": [
    "##  Decision Tree Classifier\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "a4f61ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.8697394789579158\n",
      "Testing Accuracy:  0.752\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import tree\n",
    "\n",
    "dtc_model = DecisionTreeClassifier(max_depth=5) \n",
    "dtc_model.fit(X_train, y_train)\n",
    "\n",
    "pred_train = dtc_model.predict(X_train)\n",
    "acc_train = accuracy_score(y_train, pred_train)\n",
    "print(\"Training Accuracy: \", acc_train)\n",
    "\n",
    "pred_test = dtc_model.predict(X_test)\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "print(\"Testing Accuracy: \", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "00136522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search (GridSearchCV) tries parameters exhaustively and is precise but slow\n",
    "# random search (RandomizedSearchCV) tries parameters randomly and is less precise but much faster\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'criterion':('gini', 'entropy'), \n",
    "         'splitter':('best', 'random'), \n",
    "         'max_depth' : [1,2,3,4,5,10], \n",
    "         'min_samples_leaf' : [1, 2, 3, 4, 5, 10, 100]}\n",
    "\n",
    "DecisionTreeClassifierAlgorithm = DecisionTreeClassifier() \n",
    "ModelGridSearch = GridSearchCV(DecisionTreeClassifierAlgorithm, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "f92a90cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'criterion': ('gini', 'entropy'),\n",
       "                         'max_depth': [1, 2, 3, 4, 5, 10],\n",
       "                         'min_samples_leaf': [1, 2, 3, 4, 5, 10, 100],\n",
       "                         'splitter': ('best', 'random')})"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelGridSearch.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "fd8ee8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_leaf=5)"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "40c8affa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.8697394789579158\n",
      "Testing Accuracy:  0.752\n"
     ]
    }
   ],
   "source": [
    "pred_train = dtc_model.predict(X_train)\n",
    "acc_train = accuracy_score(y_train, pred_train)\n",
    "print(\"Training Accuracy: \", acc_train)\n",
    "\n",
    "pred_test = dtc_model.predict(X_test)\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "print(\"Testing Accuracy: \", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "dcd74a18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtc_result = pd.DataFrame({'Model': ['Decision Tree'], 'Train Accuracy': [acc_train], 'Test Accuracy': [acc_test]})\n",
    "# results.append(dict1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "740537d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.869739</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Train Accuracy  Test Accuracy\n",
       "0  Decision Tree        0.869739          0.752"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56bb60",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "A Bagging model works by dividing a dataset, training one tree per subset, and averaging the results. There are variants of the Bagging approach with distinct names determined by how the data is divided (i.e. Pasting, Bagging, Random Subspaces, Random Patches, etc...).\n",
    "\n",
    "Bagging is an ensemble algorithm, in that multiple models are combined to produce a net result that outperforms any of the individual models. This approach can significantly reduce the amount of variance in the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "823cad08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.8817635270541082\n",
      "Testing Accuracy:  0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    2.6s remaining:    8.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Decision Trees Classifier using Bagging\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# we are training the same tree but using bagging\n",
    "# set the number of estimators to the number of CPUs on your computer\n",
    "\n",
    "# used best decision tree classifier previously estimated\n",
    "mb = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=10, min_samples_leaf=10),\n",
    "#                        n_estimators=, default 10\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1)\n",
    "\n",
    "mb.fit(X_train, y_train)\n",
    "\n",
    "pred_train = mb.predict(X_train)\n",
    "acc_train = accuracy_score(y_train, pred_train)\n",
    "print(\"Training Accuracy: \", acc_train)\n",
    "\n",
    "pred_test = mb.predict(X_test)\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "print(\"Testing Accuracy: \", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "77ea6e22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bdtc_result = pd.DataFrame({'Model': ['Bagging Decision Tree'], 'Train Accuracy': [acc_train], 'Test Accuracy': [acc_test]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "4d5e55ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = dtc_result.append(bdtc_result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "98ee7b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.869739</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging Decision Tree</td>\n",
       "      <td>0.881764</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Train Accuracy  Test Accuracy\n",
       "0          Decision Tree        0.869739          0.752\n",
       "1  Bagging Decision Tree        0.881764          0.800"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2738291b",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Random forest is a supervised machine learning algorithm used to solve classification as well as regression problems. It is a type of ensemble learning technique in which multiple decision trees are created from the training dataset and the majority output from them is considered as the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "42c5bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "fa181f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9559118236472945\n",
      "Testing Accuracy:  0.856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "rfc_model = RandomForestClassifier(max_depth=7, verbose=1, n_jobs=-1)\n",
    "\n",
    "rfc_model.fit(X_train, y_train)\n",
    "\n",
    "pred_train = rfc_model.predict(X_train)\n",
    "acc_train = accuracy_score(y_train, pred_train)\n",
    "print(\"Training Accuracy: \", acc_train)\n",
    "\n",
    "pred_test = rfc_model.predict(X_test)\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "print(\"Testing Accuracy: \", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "76be76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'criterion':('gini', 'entropy'),\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20, 30],\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30]}\n",
    "rand = RandomForestClassifier()\n",
    "RandomGridSearch = GridSearchCV(rand, param, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "9708055f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'criterion': ('gini', 'entropy'),\n",
       "                         'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15,\n",
       "                                       20, 30],\n",
       "                         'n_estimators': [5, 10, 15, 20, 25, 30]})"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomGridSearch.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "b0d5186e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, n_estimators=30)"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "3e12c4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9158316633266533\n",
      "Testing Accuracy:  0.848\n"
     ]
    }
   ],
   "source": [
    "pred_train = RandomGridSearch.predict(X_train)\n",
    "acc_train = accuracy_score(y_train, pred_train)\n",
    "print(\"Training Accuracy: \", acc_train)\n",
    "\n",
    "pred_test = RandomGridSearch.predict(X_test)\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "print(\"Testing Accuracy: \", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "31ac0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_result = pd.DataFrame({'Model': ['Random Forest Tree'], 'Train Accuracy': [acc_train], 'Test Accuracy': [acc_test]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "aa5a1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r1.append(rfc_result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "2c45ca04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.869739</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging Decision Tree</td>\n",
       "      <td>0.881764</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest Tree</td>\n",
       "      <td>0.915832</td>\n",
       "      <td>0.848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Train Accuracy  Test Accuracy\n",
       "0          Decision Tree        0.869739          0.752\n",
       "1  Bagging Decision Tree        0.881764          0.800\n",
       "2     Random Forest Tree        0.915832          0.848"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5990c634",
   "metadata": {},
   "source": [
    "## Gradient Tree Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "f88cadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "ba7fa9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package xgboost:\n",
      "\n",
      "NAME\n",
      "    xgboost - XGBoost: eXtreme Gradient Boosting library.\n",
      "\n",
      "DESCRIPTION\n",
      "    Contributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    callback\n",
      "    compat\n",
      "    config\n",
      "    core\n",
      "    dask\n",
      "    data\n",
      "    libpath\n",
      "    plotting\n",
      "    rabit\n",
      "    sklearn\n",
      "    tracker\n",
      "    training\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        xgboost.core.Booster\n",
      "        xgboost.core.DMatrix\n",
      "            xgboost.core.DeviceQuantileDMatrix\n",
      "        xgboost.core.DataIter\n",
      "        xgboost.tracker.RabitTracker\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        xgboost.sklearn.XGBModel\n",
      "            xgboost.sklearn.XGBClassifier(xgboost.sklearn.XGBModel, sklearn.base.ClassifierMixin)\n",
      "                xgboost.sklearn.XGBRFClassifier\n",
      "            xgboost.sklearn.XGBRanker(xgboost.sklearn.XGBModel, xgboost.sklearn.XGBRankerMixIn)\n",
      "            xgboost.sklearn.XGBRegressor(xgboost.sklearn.XGBModel, sklearn.base.RegressorMixin)\n",
      "                xgboost.sklearn.XGBRFRegressor\n",
      "    \n",
      "    class Booster(builtins.object)\n",
      "     |  Booster(params=None, cache=(), model_file=None)\n",
      "     |  \n",
      "     |  A Booster of XGBoost.\n",
      "     |  \n",
      "     |  Booster is the model of xgboost, that contains low level routines for\n",
      "     |  training, prediction and evaluation.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __copy__(self)\n",
      "     |  \n",
      "     |  __deepcopy__(self, _)\n",
      "     |      Return a copy of booster.\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |  \n",
      "     |  __getitem__(self, val)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __init__(self, params=None, cache=(), model_file=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      params : dict\n",
      "     |          Parameters for boosters.\n",
      "     |      cache : list\n",
      "     |          List of cache items.\n",
      "     |      model_file : string/os.PathLike/Booster/bytearray\n",
      "     |          Path to the model file if it's string or PathLike.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  attr(self, key)\n",
      "     |      Get attribute string from the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : str\n",
      "     |          The key to get attribute from.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      value : str\n",
      "     |          The attribute value of the key, returns None if attribute do not exist.\n",
      "     |  \n",
      "     |  attributes(self)\n",
      "     |      Get attributes stored in the Booster as a dictionary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : dictionary of  attribute_name: attribute_value pairs of strings.\n",
      "     |          Returns an empty dict if there's no attributes.\n",
      "     |  \n",
      "     |  boost(self, dtrain, grad, hess)\n",
      "     |      Boost the booster for one iteration, with customized gradient\n",
      "     |      statistics.  Like :py:func:`xgboost.Booster.update`, this\n",
      "     |      function should not be called directly by users.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dtrain : DMatrix\n",
      "     |          The training DMatrix.\n",
      "     |      grad : list\n",
      "     |          The first order of gradient.\n",
      "     |      hess : list\n",
      "     |          The second order of gradient.\n",
      "     |  \n",
      "     |  copy(self)\n",
      "     |      Copy the booster object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster: `Booster`\n",
      "     |          a copied booster model\n",
      "     |  \n",
      "     |  dump_model(self, fout, fmap='', with_stats=False, dump_format='text')\n",
      "     |      Dump model into a text or JSON file.  Unlike `save_model`, the\n",
      "     |      output format is primarily used for visualization or interpretation,\n",
      "     |      hence it's more human readable but cannot be loaded back to XGBoost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fout : string or os.PathLike\n",
      "     |          Output file name.\n",
      "     |      fmap : string or os.PathLike, optional\n",
      "     |          Name of the file containing feature map names.\n",
      "     |      with_stats : bool, optional\n",
      "     |          Controls whether the split statistics are output.\n",
      "     |      dump_format : string, optional\n",
      "     |          Format of model dump file. Can be 'text' or 'json'.\n",
      "     |  \n",
      "     |  eval(self, data, name='eval', iteration=0)\n",
      "     |      Evaluate the model on mat.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      \n",
      "     |      name : str, optional\n",
      "     |          The name of the dataset.\n",
      "     |      \n",
      "     |      iteration : int, optional\n",
      "     |          The current iteration number.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result: str\n",
      "     |          Evaluation result string.\n",
      "     |  \n",
      "     |  eval_set(self, evals, iteration=0, feval=None)\n",
      "     |      Evaluate a set of data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      evals : list of tuples (DMatrix, string)\n",
      "     |          List of items to be evaluated.\n",
      "     |      iteration : int\n",
      "     |          Current iteration.\n",
      "     |      feval : function\n",
      "     |          Custom evaluation function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result: str\n",
      "     |          Evaluation result string.\n",
      "     |  \n",
      "     |  get_dump(self, fmap='', with_stats=False, dump_format='text')\n",
      "     |      Returns the model dump as a list of strings.  Unlike `save_model`, the\n",
      "     |      output format is primarily used for visualization or interpretation,\n",
      "     |      hence it's more human readable but cannot be loaded back to XGBoost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap : string or os.PathLike, optional\n",
      "     |          Name of the file containing feature map names.\n",
      "     |      with_stats : bool, optional\n",
      "     |          Controls whether the split statistics are output.\n",
      "     |      dump_format : string, optional\n",
      "     |          Format of model dump. Can be 'text', 'json' or 'dot'.\n",
      "     |  \n",
      "     |  get_fscore(self, fmap='')\n",
      "     |      Get feature importance of each feature.\n",
      "     |      \n",
      "     |      .. note:: Zero-importance features will not be included\n",
      "     |      \n",
      "     |         Keep in mind that this function does not include zero-importance feature, i.e.\n",
      "     |         those features that have not been used in any split conditions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap: str or os.PathLike (optional)\n",
      "     |         The name of feature map file\n",
      "     |  \n",
      "     |  get_score(self, fmap: Union[str, os.PathLike] = '', importance_type: str = 'weight') -> Dict[str, Union[float, List[float]]]\n",
      "     |      Get feature importance of each feature.\n",
      "     |      For tree model Importance type can be defined as:\n",
      "     |      \n",
      "     |      * 'weight': the number of times a feature is used to split the data across all trees.\n",
      "     |      * 'gain': the average gain across all splits the feature is used in.\n",
      "     |      * 'cover': the average coverage across all splits the feature is used in.\n",
      "     |      * 'total_gain': the total gain across all splits the feature is used in.\n",
      "     |      * 'total_cover': the total coverage across all splits the feature is used in.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |      \n",
      "     |         For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
      "     |         without bias.\n",
      "     |      \n",
      "     |      .. note:: Zero-importance features will not be included\n",
      "     |      \n",
      "     |         Keep in mind that this function does not include zero-importance feature, i.e.\n",
      "     |         those features that have not been used in any split conditions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap: str or os.PathLike (optional)\n",
      "     |         The name of feature map file.\n",
      "     |      importance_type: str, default 'weight'\n",
      "     |          One of the importance types defined above.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A map between feature names and their scores.  When `gblinear` is used for\n",
      "     |      multi-class classification the scores for each feature is a list with length\n",
      "     |      `n_classes`, otherwise they're scalars.\n",
      "     |  \n",
      "     |  get_split_value_histogram(self, feature: str, fmap: Union[os.PathLike, str] = '', bins: Optional[int] = None, as_pandas: bool = True) -> Union[numpy.ndarray, pandas.core.frame.DataFrame]\n",
      "     |      Get split value histogram of a feature\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      feature: str\n",
      "     |          The name of the feature.\n",
      "     |      fmap: str or os.PathLike (optional)\n",
      "     |          The name of feature map file.\n",
      "     |      bin: int, default None\n",
      "     |          The maximum number of bins.\n",
      "     |          Number of bins equals number of unique split values n_unique,\n",
      "     |          if bins == None or bins > n_unique.\n",
      "     |      as_pandas: bool, default True\n",
      "     |          Return pd.DataFrame when pandas is installed.\n",
      "     |          If False or pandas is not installed, return numpy ndarray.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a histogram of used splitting values for the specified feature\n",
      "     |      either as numpy array or pandas DataFrame.\n",
      "     |  \n",
      "     |  inplace_predict(self, data: Any, iteration_range: Tuple[int, int] = (0, 0), predict_type: str = 'value', missing: float = nan, validate_features: bool = True, base_margin: Any = None, strict_shape: bool = False)\n",
      "     |      Run prediction in-place, Unlike ``predict`` method, inplace prediction does\n",
      "     |      not cache the prediction result.\n",
      "     |      \n",
      "     |      Calling only ``inplace_predict`` in multiple threads is safe and lock\n",
      "     |      free.  But the safety does not hold when used in conjunction with other\n",
      "     |      methods. E.g. you can't train the booster in one thread and perform\n",
      "     |      prediction in the other.\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          booster.set_param({'predictor': 'gpu_predictor'})\n",
      "     |          booster.inplace_predict(cupy_array)\n",
      "     |      \n",
      "     |          booster.set_param({'predictor': 'cpu_predictor})\n",
      "     |          booster.inplace_predict(numpy_array)\n",
      "     |      \n",
      "     |      .. versionadded:: 1.1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : numpy.ndarray/scipy.sparse.csr_matrix/cupy.ndarray/\n",
      "     |             cudf.DataFrame/pd.DataFrame\n",
      "     |          The input data, must not be a view for numpy array.  Set\n",
      "     |          ``predictor`` to ``gpu_predictor`` for running prediction on CuPy\n",
      "     |          array or CuDF DataFrame.\n",
      "     |      iteration_range :\n",
      "     |          See :py:meth:`xgboost.Booster.predict` for details.\n",
      "     |      predict_type :\n",
      "     |          * `value` Output model prediction values.\n",
      "     |          * `margin` Output the raw untransformed margin value.\n",
      "     |      missing :\n",
      "     |          See :py:obj:`xgboost.DMatrix` for details.\n",
      "     |      validate_features:\n",
      "     |          See :py:meth:`xgboost.Booster.predict` for details.\n",
      "     |      base_margin:\n",
      "     |          See :py:obj:`xgboost.DMatrix` for details.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      strict_shape:\n",
      "     |          See :py:meth:`xgboost.Booster.predict` for details.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy.ndarray/cupy.ndarray\n",
      "     |          The prediction result.  When input data is on GPU, prediction\n",
      "     |          result is stored in a cupy array.\n",
      "     |  \n",
      "     |  load_config(self, config)\n",
      "     |      Load configuration returned by `save_config`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |  \n",
      "     |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n",
      "     |      Load the model from a file or bytearray. Path to file can be local\n",
      "     |      or as an URI.\n",
      "     |      \n",
      "     |      The model is loaded from XGBoost format which is universal among the various\n",
      "     |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n",
      "     |      feature_names) will not be loaded when using binary format.  To save those\n",
      "     |      attributes, use JSON instead.  See: `Model IO\n",
      "     |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      "     |      info.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname :\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  num_boosted_rounds(self) -> int\n",
      "     |      Get number of boosted rounds.  For gblinear this is reset to 0 after\n",
      "     |      serializing the model.\n",
      "     |  \n",
      "     |  num_features(self) -> int\n",
      "     |      Number of features in booster.\n",
      "     |  \n",
      "     |  predict(self, data: xgboost.core.DMatrix, output_margin: bool = False, ntree_limit: int = 0, pred_leaf: bool = False, pred_contribs: bool = False, approx_contribs: bool = False, pred_interactions: bool = False, validate_features: bool = True, training: bool = False, iteration_range: Tuple[int, int] = (0, 0), strict_shape: bool = False) -> numpy.ndarray\n",
      "     |      Predict with data.  The full model will be used unless `iteration_range` is specified,\n",
      "     |      meaning user have to either slice the model or use the ``best_iteration``\n",
      "     |      attribute to get prediction from best model returned from early stopping.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |      \n",
      "     |          See `Prediction\n",
      "     |          <https://xgboost.readthedocs.io/en/latest/prediction.html>`_\n",
      "     |          for issues like thread safety and a summary of outputs from this function.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data :\n",
      "     |          The dmatrix storing the input.\n",
      "     |      \n",
      "     |      output_margin :\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      \n",
      "     |      ntree_limit :\n",
      "     |          Deprecated, use `iteration_range` instead.\n",
      "     |      \n",
      "     |      pred_leaf :\n",
      "     |          When this option is on, the output will be a matrix of (nsample,\n",
      "     |          ntrees) with each record indicating the predicted leaf index of\n",
      "     |          each sample in each tree.  Note that the leaf index of a tree is\n",
      "     |          unique per tree, so you may find leaf 1 in both tree 1 and tree 0.\n",
      "     |      \n",
      "     |      pred_contribs :\n",
      "     |          When this is True the output will be a matrix of size (nsample,\n",
      "     |          nfeats + 1) with each record indicating the feature contributions\n",
      "     |          (SHAP values) for that prediction. The sum of all feature\n",
      "     |          contributions is equal to the raw untransformed margin value of the\n",
      "     |          prediction. Note the final column is the bias term.\n",
      "     |      \n",
      "     |      approx_contribs :\n",
      "     |          Approximate the contributions of each feature.  Used when ``pred_contribs`` or\n",
      "     |          ``pred_interactions`` is set to True.  Changing the default of this parameter\n",
      "     |          (False) is not recommended.\n",
      "     |      \n",
      "     |      pred_interactions :\n",
      "     |          When this is True the output will be a matrix of size (nsample,\n",
      "     |          nfeats + 1, nfeats + 1) indicating the SHAP interaction values for\n",
      "     |          each pair of features. The sum of each row (or column) of the\n",
      "     |          interaction values equals the corresponding SHAP value (from\n",
      "     |          pred_contribs), and the sum of the entire matrix equals the raw\n",
      "     |          untransformed margin value of the prediction. Note the last row and\n",
      "     |          column correspond to the bias term.\n",
      "     |      \n",
      "     |      validate_features :\n",
      "     |          When this is True, validate that the Booster's and data's\n",
      "     |          feature_names are identical.  Otherwise, it is assumed that the\n",
      "     |          feature_names are the same.\n",
      "     |      \n",
      "     |      training :\n",
      "     |          Whether the prediction value is used for training.  This can effect `dart`\n",
      "     |          booster, which performs dropouts during training iterations but use all trees\n",
      "     |          for inference. If you want to obtain result with dropouts, set this parameter\n",
      "     |          to `True`.  Also, the parameter is set to true when obtaining prediction for\n",
      "     |          custom objective function.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.0.0\n",
      "     |      \n",
      "     |      iteration_range :\n",
      "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      "     |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
      "     |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
      "     |          used in this prediction.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      strict_shape :\n",
      "     |          When set to True, output shape is invariant to whether classification is used.\n",
      "     |          For both value and margin prediction, the output shape is (n_samples,\n",
      "     |          n_groups), n_groups == 1 when multi-class is not used.  Default to False, in\n",
      "     |          which case the output shape can be (n_samples, ) if multi-class is not used.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_config(self)\n",
      "     |      Output internal parameter configuration of Booster as a JSON\n",
      "     |      string.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0.0\n",
      "     |  \n",
      "     |  save_model(self, fname: Union[str, os.PathLike])\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal format which is universal among the\n",
      "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
      "     |      (such as feature_names) will not be saved when using binary format.  To save those\n",
      "     |      attributes, use JSON instead. See: `Model IO\n",
      "     |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      "     |      info.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or os.PathLike\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  save_raw(self)\n",
      "     |      Save the model to a in memory buffer representation instead of file.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a in memory buffer representation of the model\n",
      "     |  \n",
      "     |  set_attr(self, **kwargs: Optional[str]) -> None\n",
      "     |      Set the attribute of the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs\n",
      "     |          The attributes to set. Setting a value to None deletes an attribute.\n",
      "     |  \n",
      "     |  set_param(self, params, value=None)\n",
      "     |      Set parameters into the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      params: dict/list/str\n",
      "     |         list of key,value pairs, dict of key to value or simply str key\n",
      "     |      value: optional\n",
      "     |         value of the specified parameter, when params is str key\n",
      "     |  \n",
      "     |  trees_to_dataframe(self, fmap='')\n",
      "     |      Parse a boosted tree model text dump into a pandas DataFrame structure.\n",
      "     |      \n",
      "     |      This feature is only defined when the decision tree model is chosen as base\n",
      "     |      learner (`booster in {gbtree, dart}`). It is not defined for other base learner\n",
      "     |      types, such as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap: str or os.PathLike (optional)\n",
      "     |         The name of feature map file.\n",
      "     |  \n",
      "     |  update(self, dtrain, iteration, fobj=None)\n",
      "     |      Update for one iteration, with objective function calculated\n",
      "     |      internally.  This function should not be called directly by users.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dtrain : DMatrix\n",
      "     |          Training data.\n",
      "     |      iteration : int\n",
      "     |          Current iteration number.\n",
      "     |      fobj : function\n",
      "     |          Customized objective function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  feature_names\n",
      "     |      Feature names for this booster.  Can be directly set by input data or by\n",
      "     |      assignment.\n",
      "     |  \n",
      "     |  feature_types\n",
      "     |      Feature types for this booster.  Can be directly set by input data or by\n",
      "     |      assignment.\n",
      "    \n",
      "    class DMatrix(builtins.object)\n",
      "     |  DMatrix(data, label=None, *, weight=None, base_margin=None, missing: Optional[float] = None, silent=False, feature_names: Optional[List[str]] = None, feature_types: Optional[List[str]] = None, nthread: Optional[int] = None, group=None, qid=None, label_lower_bound=None, label_upper_bound=None, feature_weights=None, enable_categorical: bool = False) -> None\n",
      "     |  \n",
      "     |  Data Matrix used in XGBoost.\n",
      "     |  \n",
      "     |  DMatrix is an internal data structure that is used by XGBoost,\n",
      "     |  which is optimized for both memory efficiency and training speed.\n",
      "     |  You can construct DMatrix from multiple different sources of data.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |  \n",
      "     |  __init__(self, data, label=None, *, weight=None, base_margin=None, missing: Optional[float] = None, silent=False, feature_names: Optional[List[str]] = None, feature_types: Optional[List[str]] = None, nthread: Optional[int] = None, group=None, qid=None, label_lower_bound=None, label_upper_bound=None, feature_weights=None, enable_categorical: bool = False) -> None\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : os.PathLike/string/numpy.array/scipy.sparse/pd.DataFrame/\n",
      "     |             dt.Frame/cudf.DataFrame/cupy.array/dlpack\n",
      "     |          Data source of DMatrix.\n",
      "     |          When data is string or os.PathLike type, it represents the path\n",
      "     |          libsvm format txt file, csv file (by specifying uri parameter\n",
      "     |          'path_to_csv?format=csv'), or binary file that xgboost can read\n",
      "     |          from.\n",
      "     |      label : array_like\n",
      "     |          Label of the training data.\n",
      "     |      weight : array_like\n",
      "     |          Weight for each instance.\n",
      "     |      \n",
      "     |          .. note:: For ranking task, weights are per-group.\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each group (not each\n",
      "     |              data point). This is because we only care about the relative\n",
      "     |              ordering of data points within each group, so it doesn't make\n",
      "     |              sense to assign weights to individual data points.\n",
      "     |      \n",
      "     |      base_margin: array_like\n",
      "     |          Base margin used for boosting from existing model.\n",
      "     |      missing : float, optional\n",
      "     |          Value in the input data which needs to be present as a missing\n",
      "     |          value. If None, defaults to np.nan.\n",
      "     |      silent : boolean, optional\n",
      "     |          Whether print messages during construction\n",
      "     |      feature_names : list, optional\n",
      "     |          Set names for features.\n",
      "     |      feature_types :\n",
      "     |      \n",
      "     |          Set types for features.  When `enable_categorical` is set to `True`, string\n",
      "     |          \"c\" represents categorical data type.\n",
      "     |      \n",
      "     |      nthread : integer, optional\n",
      "     |          Number of threads to use for loading data when parallelization is\n",
      "     |          applicable. If -1, uses maximum threads available on the system.\n",
      "     |      group : array_like\n",
      "     |          Group size for all ranking group.\n",
      "     |      qid : array_like\n",
      "     |          Query ID for data samples, used for ranking.\n",
      "     |      label_lower_bound : array_like\n",
      "     |          Lower bound for survival training.\n",
      "     |      label_upper_bound : array_like\n",
      "     |          Upper bound for survival training.\n",
      "     |      feature_weights : array_like, optional\n",
      "     |          Set feature weights for column sampling.\n",
      "     |      enable_categorical: boolean, optional\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |          Experimental support of specializing for categorical features.  Do not set to\n",
      "     |          True unless you are interested in development.  Currently it's only available\n",
      "     |          for `gpu_hist` tree method with 1 vs rest (one hot) categorical split.  Also,\n",
      "     |          JSON serialization format is required.\n",
      "     |  \n",
      "     |  get_base_margin(self)\n",
      "     |      Get the base margin of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      base_margin : float\n",
      "     |  \n",
      "     |  get_float_info(self, field)\n",
      "     |      Get float property from the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      info : array\n",
      "     |          a numpy array of float information of the data\n",
      "     |  \n",
      "     |  get_label(self)\n",
      "     |      Get the label of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      label : array\n",
      "     |  \n",
      "     |  get_uint_info(self, field)\n",
      "     |      Get unsigned integer property from the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      info : array\n",
      "     |          a numpy array of unsigned integer information of the data\n",
      "     |  \n",
      "     |  get_weight(self)\n",
      "     |      Get the weight of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      weight : array\n",
      "     |  \n",
      "     |  num_col(self)\n",
      "     |      Get the number of columns (features) in the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      number of columns : int\n",
      "     |  \n",
      "     |  num_row(self)\n",
      "     |      Get the number of rows in the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      number of rows : int\n",
      "     |  \n",
      "     |  save_binary(self, fname, silent=True)\n",
      "     |      Save DMatrix to an XGBoost buffer.  Saved binary can be later loaded\n",
      "     |      by providing the path to :py:func:`xgboost.DMatrix` as input.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or os.PathLike\n",
      "     |          Name of the output buffer file.\n",
      "     |      silent : bool (optional; default: True)\n",
      "     |          If set, the output is suppressed.\n",
      "     |  \n",
      "     |  set_base_margin(self, margin)\n",
      "     |      Set base margin of booster to start from.\n",
      "     |      \n",
      "     |      This can be used to specify a prediction value of existing model to be\n",
      "     |      base_margin However, remember margin is needed, instead of transformed\n",
      "     |      prediction e.g. for logistic regression: need to put in value before\n",
      "     |      logistic transformation see also example/demo.py\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      margin: array like\n",
      "     |          Prediction margin of each datapoint\n",
      "     |  \n",
      "     |  set_float_info(self, field, data)\n",
      "     |      Set float type property into the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_float_info_npy2d(self, field, data)\n",
      "     |      Set float type property into the DMatrix\n",
      "     |         for numpy 2d array input\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_group(self, group)\n",
      "     |      Set group size of DMatrix (used for ranking).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      group : array like\n",
      "     |          Group size of each group\n",
      "     |  \n",
      "     |  set_info(self, *, label=None, weight=None, base_margin=None, group=None, qid=None, label_lower_bound=None, label_upper_bound=None, feature_names: Optional[List[str]] = None, feature_types: Optional[List[str]] = None, feature_weights=None) -> None\n",
      "     |      Set meta info for DMatrix.  See doc string for :py:obj:`xgboost.DMatrix`.\n",
      "     |  \n",
      "     |  set_label(self, label)\n",
      "     |      Set label of dmatrix\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      label: array like\n",
      "     |          The label information to be set into DMatrix\n",
      "     |  \n",
      "     |  set_uint_info(self, field, data)\n",
      "     |      Set uint type property into the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_weight(self, weight)\n",
      "     |      Set weight of each instance.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      weight : array like\n",
      "     |          Weight for each data point\n",
      "     |      \n",
      "     |          .. note:: For ranking task, weights are per-group.\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each group (not each\n",
      "     |              data point). This is because we only care about the relative\n",
      "     |              ordering of data points within each group, so it doesn't make\n",
      "     |              sense to assign weights to individual data points.\n",
      "     |  \n",
      "     |  slice(self, rindex: Union[List[int], numpy.ndarray], allow_groups: bool = False) -> 'DMatrix'\n",
      "     |      Slice the DMatrix and return a new DMatrix that only contains `rindex`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      rindex\n",
      "     |          List of indices to be selected.\n",
      "     |      allow_groups\n",
      "     |          Allow slicing of a matrix with a groups attribute\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      res\n",
      "     |          A new DMatrix containing only selected indices.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  feature_names\n",
      "     |      Get feature names (column labels).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names : list or None\n",
      "     |  \n",
      "     |  feature_types\n",
      "     |      Get feature types (column types).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_types : list or None\n",
      "    \n",
      "    class DataIter(builtins.object)\n",
      "     |  DataIter(cache_prefix: Optional[str] = None) -> None\n",
      "     |  \n",
      "     |  The interface for user defined data iterator.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  cache_prefix:\n",
      "     |      Prefix to the cache files, only used in external memory.  It can be either an URI\n",
      "     |      or a file path.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __del__(self) -> None\n",
      "     |  \n",
      "     |  __init__(self, cache_prefix: Optional[str] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  next(self, input_data: Callable) -> int\n",
      "     |      Set the next batch of data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      \n",
      "     |      data_handle:\n",
      "     |          A function with same data fields like `data`, `label` with\n",
      "     |          `xgboost.DMatrix`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      0 if there's no more batch, otherwise 1.\n",
      "     |  \n",
      "     |  reset(self) -> None\n",
      "     |      Reset the data iterator.  Prototype for user defined function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  proxy\n",
      "     |      Handle of DMatrix proxy.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DeviceQuantileDMatrix(DMatrix)\n",
      "     |  DeviceQuantileDMatrix(data, label=None, *, weight=None, base_margin=None, missing=None, silent=False, feature_names=None, feature_types=None, nthread: Optional[int] = None, max_bin: int = 256, group=None, qid=None, label_lower_bound=None, label_upper_bound=None, feature_weights=None, enable_categorical: bool = False)\n",
      "     |  \n",
      "     |  Device memory Data Matrix used in XGBoost for training with tree_method='gpu_hist'. Do\n",
      "     |  not use this for test/validation tasks as some information may be lost in\n",
      "     |  quantisation. This DMatrix is primarily designed to save memory in training from\n",
      "     |  device memory inputs by avoiding intermediate storage. Set max_bin to control the\n",
      "     |  number of bins during quantisation.  See doc string in :py:obj:`xgboost.DMatrix` for\n",
      "     |  documents on meta info.\n",
      "     |  \n",
      "     |  You can construct DeviceQuantileDMatrix from cupy/cudf/dlpack.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.1.0\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DeviceQuantileDMatrix\n",
      "     |      DMatrix\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, data, label=None, *, weight=None, base_margin=None, missing=None, silent=False, feature_names=None, feature_types=None, nthread: Optional[int] = None, max_bin: int = 256, group=None, qid=None, label_lower_bound=None, label_upper_bound=None, feature_weights=None, enable_categorical: bool = False)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : os.PathLike/string/numpy.array/scipy.sparse/pd.DataFrame/\n",
      "     |             dt.Frame/cudf.DataFrame/cupy.array/dlpack\n",
      "     |          Data source of DMatrix.\n",
      "     |          When data is string or os.PathLike type, it represents the path\n",
      "     |          libsvm format txt file, csv file (by specifying uri parameter\n",
      "     |          'path_to_csv?format=csv'), or binary file that xgboost can read\n",
      "     |          from.\n",
      "     |      label : array_like\n",
      "     |          Label of the training data.\n",
      "     |      weight : array_like\n",
      "     |          Weight for each instance.\n",
      "     |      \n",
      "     |          .. note:: For ranking task, weights are per-group.\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each group (not each\n",
      "     |              data point). This is because we only care about the relative\n",
      "     |              ordering of data points within each group, so it doesn't make\n",
      "     |              sense to assign weights to individual data points.\n",
      "     |      \n",
      "     |      base_margin: array_like\n",
      "     |          Base margin used for boosting from existing model.\n",
      "     |      missing : float, optional\n",
      "     |          Value in the input data which needs to be present as a missing\n",
      "     |          value. If None, defaults to np.nan.\n",
      "     |      silent : boolean, optional\n",
      "     |          Whether print messages during construction\n",
      "     |      feature_names : list, optional\n",
      "     |          Set names for features.\n",
      "     |      feature_types :\n",
      "     |      \n",
      "     |          Set types for features.  When `enable_categorical` is set to `True`, string\n",
      "     |          \"c\" represents categorical data type.\n",
      "     |      \n",
      "     |      nthread : integer, optional\n",
      "     |          Number of threads to use for loading data when parallelization is\n",
      "     |          applicable. If -1, uses maximum threads available on the system.\n",
      "     |      group : array_like\n",
      "     |          Group size for all ranking group.\n",
      "     |      qid : array_like\n",
      "     |          Query ID for data samples, used for ranking.\n",
      "     |      label_lower_bound : array_like\n",
      "     |          Lower bound for survival training.\n",
      "     |      label_upper_bound : array_like\n",
      "     |          Upper bound for survival training.\n",
      "     |      feature_weights : array_like, optional\n",
      "     |          Set feature weights for column sampling.\n",
      "     |      enable_categorical: boolean, optional\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3.0\n",
      "     |      \n",
      "     |          Experimental support of specializing for categorical features.  Do not set to\n",
      "     |          True unless you are interested in development.  Currently it's only available\n",
      "     |          for `gpu_hist` tree method with 1 vs rest (one hot) categorical split.  Also,\n",
      "     |          JSON serialization format is required.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DMatrix:\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |  \n",
      "     |  get_base_margin(self)\n",
      "     |      Get the base margin of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      base_margin : float\n",
      "     |  \n",
      "     |  get_float_info(self, field)\n",
      "     |      Get float property from the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      info : array\n",
      "     |          a numpy array of float information of the data\n",
      "     |  \n",
      "     |  get_label(self)\n",
      "     |      Get the label of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      label : array\n",
      "     |  \n",
      "     |  get_uint_info(self, field)\n",
      "     |      Get unsigned integer property from the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      info : array\n",
      "     |          a numpy array of unsigned integer information of the data\n",
      "     |  \n",
      "     |  get_weight(self)\n",
      "     |      Get the weight of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      weight : array\n",
      "     |  \n",
      "     |  num_col(self)\n",
      "     |      Get the number of columns (features) in the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      number of columns : int\n",
      "     |  \n",
      "     |  num_row(self)\n",
      "     |      Get the number of rows in the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      number of rows : int\n",
      "     |  \n",
      "     |  save_binary(self, fname, silent=True)\n",
      "     |      Save DMatrix to an XGBoost buffer.  Saved binary can be later loaded\n",
      "     |      by providing the path to :py:func:`xgboost.DMatrix` as input.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or os.PathLike\n",
      "     |          Name of the output buffer file.\n",
      "     |      silent : bool (optional; default: True)\n",
      "     |          If set, the output is suppressed.\n",
      "     |  \n",
      "     |  set_base_margin(self, margin)\n",
      "     |      Set base margin of booster to start from.\n",
      "     |      \n",
      "     |      This can be used to specify a prediction value of existing model to be\n",
      "     |      base_margin However, remember margin is needed, instead of transformed\n",
      "     |      prediction e.g. for logistic regression: need to put in value before\n",
      "     |      logistic transformation see also example/demo.py\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      margin: array like\n",
      "     |          Prediction margin of each datapoint\n",
      "     |  \n",
      "     |  set_float_info(self, field, data)\n",
      "     |      Set float type property into the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_float_info_npy2d(self, field, data)\n",
      "     |      Set float type property into the DMatrix\n",
      "     |         for numpy 2d array input\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_group(self, group)\n",
      "     |      Set group size of DMatrix (used for ranking).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      group : array like\n",
      "     |          Group size of each group\n",
      "     |  \n",
      "     |  set_info(self, *, label=None, weight=None, base_margin=None, group=None, qid=None, label_lower_bound=None, label_upper_bound=None, feature_names: Optional[List[str]] = None, feature_types: Optional[List[str]] = None, feature_weights=None) -> None\n",
      "     |      Set meta info for DMatrix.  See doc string for :py:obj:`xgboost.DMatrix`.\n",
      "     |  \n",
      "     |  set_label(self, label)\n",
      "     |      Set label of dmatrix\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      label: array like\n",
      "     |          The label information to be set into DMatrix\n",
      "     |  \n",
      "     |  set_uint_info(self, field, data)\n",
      "     |      Set uint type property into the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_weight(self, weight)\n",
      "     |      Set weight of each instance.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      weight : array like\n",
      "     |          Weight for each data point\n",
      "     |      \n",
      "     |          .. note:: For ranking task, weights are per-group.\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each group (not each\n",
      "     |              data point). This is because we only care about the relative\n",
      "     |              ordering of data points within each group, so it doesn't make\n",
      "     |              sense to assign weights to individual data points.\n",
      "     |  \n",
      "     |  slice(self, rindex: Union[List[int], numpy.ndarray], allow_groups: bool = False) -> 'DMatrix'\n",
      "     |      Slice the DMatrix and return a new DMatrix that only contains `rindex`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      rindex\n",
      "     |          List of indices to be selected.\n",
      "     |      allow_groups\n",
      "     |          Allow slicing of a matrix with a groups attribute\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      res\n",
      "     |          A new DMatrix containing only selected indices.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DMatrix:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  feature_names\n",
      "     |      Get feature names (column labels).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names : list or None\n",
      "     |  \n",
      "     |  feature_types\n",
      "     |      Get feature types (column types).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_types : list or None\n",
      "    \n",
      "    class RabitTracker(builtins.object)\n",
      "     |  RabitTracker(hostIP, nslave, port=9091, port_end=9999, use_logger: bool = False) -> None\n",
      "     |  \n",
      "     |  tracker for rabit\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |  \n",
      "     |  __init__(self, hostIP, nslave, port=9091, port_end=9999, use_logger: bool = False) -> None\n",
      "     |      A Python implementation of RABIT tracker.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ..........\n",
      "     |      use_logger:\n",
      "     |          Use logging.info for tracker print command.  When set to False, Python print\n",
      "     |          function is used instead.\n",
      "     |  \n",
      "     |  accept_slaves(self, nslave)\n",
      "     |  \n",
      "     |  alive(self)\n",
      "     |  \n",
      "     |  find_share_ring(self, tree_map, parent_map, r)\n",
      "     |      get a ring structure that tends to share nodes with the tree\n",
      "     |      return a list starting from r\n",
      "     |  \n",
      "     |  get_link_map(self, nslave)\n",
      "     |      get the link map, this is a bit hacky, call for better algorithm\n",
      "     |      to place similar nodes together\n",
      "     |  \n",
      "     |  get_ring(self, tree_map, parent_map)\n",
      "     |      get a ring connection used to recover local data\n",
      "     |  \n",
      "     |  get_tree(self, nslave)\n",
      "     |  \n",
      "     |  join(self)\n",
      "     |  \n",
      "     |  slave_envs(self)\n",
      "     |      get enviroment variables for slaves\n",
      "     |      can be passed in as args or envs\n",
      "     |  \n",
      "     |  start(self, nslave)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  get_neighbor(rank, nslave)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      "     |  XGBClassifier(*, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', use_label_encoder: bool = True, **kwargs: Any) -> None\n",
      "     |  \n",
      "     |  Implementation of the scikit-learn API for XGBoost classification.\n",
      "     |  \n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |      n_estimators : int\n",
      "     |          Number of boosting rounds.\n",
      "     |      use_label_encoder : bool\n",
      "     |          (Deprecated) Use the label encoder from scikit-learn to encode the labels. For new\n",
      "     |          code, we recommend that you set this parameter to False.\n",
      "     |  \n",
      "     |      max_depth :  Optional[int]\n",
      "     |          Maximum tree depth for base learners.\n",
      "     |      learning_rate : Optional[float]\n",
      "     |          Boosting learning rate (xgb's \"eta\")\n",
      "     |      verbosity : Optional[int]\n",
      "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |      objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |      booster: Optional[str]\n",
      "     |          Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |      tree_method: Optional[str]\n",
      "     |          Specify which tree method to use.  Default to auto.  If this parameter\n",
      "     |          is set to default, XGBoost will choose the most conservative option\n",
      "     |          available.  It's recommended to study this option from the parameters\n",
      "     |          document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n",
      "     |      n_jobs : Optional[int]\n",
      "     |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "     |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "     |          balance the threads.  Creating thread contention will significantly slow down both\n",
      "     |          algorithms.\n",
      "     |      gamma : Optional[float]\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf\n",
      "     |          node of the tree.\n",
      "     |      min_child_weight : Optional[float]\n",
      "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |      max_delta_step : Optional[float]\n",
      "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |      subsample : Optional[float]\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      colsample_bytree : Optional[float]\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      colsample_bylevel : Optional[float]\n",
      "     |          Subsample ratio of columns for each level.\n",
      "     |      colsample_bynode : Optional[float]\n",
      "     |          Subsample ratio of columns for each split.\n",
      "     |      reg_alpha : Optional[float]\n",
      "     |          L1 regularization term on weights (xgb's alpha).\n",
      "     |      reg_lambda : Optional[float]\n",
      "     |          L2 regularization term on weights (xgb's lambda).\n",
      "     |      scale_pos_weight : Optional[float]\n",
      "     |          Balancing of positive and negative weights.\n",
      "     |      base_score : Optional[float]\n",
      "     |          The initial prediction score of all instances, global bias.\n",
      "     |      random_state : Optional[Union[numpy.random.RandomState, int]]\n",
      "     |          Random number seed.\n",
      "     |  \n",
      "     |          .. note::\n",
      "     |  \n",
      "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      "     |             it uses Hogwild algorithm.\n",
      "     |  \n",
      "     |      missing : float, default np.nan\n",
      "     |          Value in the data which needs to be present as a missing value.\n",
      "     |      num_parallel_tree: Optional[int]\n",
      "     |          Used for boosting random forest.\n",
      "     |      monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
      "     |          Constraint of variable monotonicity.  See tutorial for more\n",
      "     |          information.\n",
      "     |      interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
      "     |          Constraints for interaction representing permitted interactions.  The\n",
      "     |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "     |          [2, 3, 4]], where each inner list is a group of indices of features\n",
      "     |          that are allowed to interact with each other.  See tutorial for more\n",
      "     |          information\n",
      "     |      importance_type: Optional[str]\n",
      "     |          The feature importance type for the feature_importances\\_ property:\n",
      "     |  \n",
      "     |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      "     |            \"total_cover\".\n",
      "     |          * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
      "     |            without bias.\n",
      "     |  \n",
      "     |      gpu_id : Optional[int]\n",
      "     |          Device ordinal.\n",
      "     |      validate_parameters : Optional[bool]\n",
      "     |          Give warnings for unknown parameter.\n",
      "     |      predictor : Optional[str]\n",
      "     |          Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n",
      "     |          gpu_predictor].\n",
      "     |      enable_categorical : bool\n",
      "     |  \n",
      "     |          .. versionadded:: 1.5.0\n",
      "     |  \n",
      "     |          Experimental support for categorical data.  Do not set to true unless you are\n",
      "     |          interested in development. Only valid when `gpu_hist` and dataframe are used.\n",
      "     |  \n",
      "     |      kwargs : dict, optional\n",
      "     |          Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "     |          parameters can be found here:\n",
      "     |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "     |          dict simultaneously will result in a TypeError.\n",
      "     |  \n",
      "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "     |              that parameters passed via this argument will interact properly\n",
      "     |              with scikit-learn.\n",
      "     |  \n",
      "     |          .. note::  Custom objective function\n",
      "     |  \n",
      "     |              A custom objective function can be provided for the ``objective``\n",
      "     |              parameter. In this case, it should have the signature\n",
      "     |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |              y_true: array_like of shape [n_samples]\n",
      "     |                  The target values\n",
      "     |              y_pred: array_like of shape [n_samples]\n",
      "     |                  The predicted values\n",
      "     |  \n",
      "     |              grad: array_like of shape [n_samples]\n",
      "     |                  The value of the gradient for each sample point.\n",
      "     |              hess: array_like of shape [n_samples]\n",
      "     |                  The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBClassifier\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', use_label_encoder: bool = True, **kwargs: Any) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  evals_result(self) -> Dict[str, Dict[str, Union[List[float], List[Tuple[float, float]]]]]\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBClassifier(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[List[Tuple[Any, Any]]] = None, eval_metric: Union[str, List[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]], NoneType] = None, early_stopping_rounds: Optional[int] = None, verbose: Optional[bool] = True, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Optional[List[Any]] = None, base_margin_eval_set: Optional[List[Any]] = None, feature_weights: Optional[Any] = None, callbacks: Optional[List[xgboost.callback.TrainingCallback]] = None) -> 'XGBClassifier'\n",
      "     |      Fit gradient boosting classifier.\n",
      "     |      \n",
      "     |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
      "     |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
      "     |      pass ``xgb_model`` argument.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X :\n",
      "     |          Feature matrix\n",
      "     |      y :\n",
      "     |          Labels\n",
      "     |      sample_weight :\n",
      "     |          instance weights\n",
      "     |      base_margin :\n",
      "     |          global bias for each instance.\n",
      "     |      eval_set :\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      "     |          metrics will be computed.\n",
      "     |          Validation metrics will help us track the performance of the model.\n",
      "     |      eval_metric :\n",
      "     |          If a str, should be a built-in evaluation metric to use. See doc/parameter.rst.\n",
      "     |      \n",
      "     |          If a list of str, should be the list of multiple built-in evaluation metrics\n",
      "     |          to use.\n",
      "     |      \n",
      "     |          If callable, a custom evaluation metric. The call signature is\n",
      "     |          ``func(y_predicted, y_true)`` where ``y_true`` will be a DMatrix object such\n",
      "     |          that you may need to call the ``get_label`` method. It must return a str,\n",
      "     |          value pair where the str is a name for the evaluation and value is the value\n",
      "     |          of the evaluation function. The callable custom objective is always minimized.\n",
      "     |      early_stopping_rounds :\n",
      "     |          Activates early stopping. Validation metric needs to improve at least once in\n",
      "     |          every **early_stopping_rounds** round(s) to continue training.\n",
      "     |          Requires at least one item in **eval_set**.\n",
      "     |      \n",
      "     |          The method returns the model from the last iteration (not the best one).\n",
      "     |          If there's more than one item in **eval_set**, the last entry will be used\n",
      "     |          for early stopping.\n",
      "     |      \n",
      "     |          If there's more than one metric in **eval_metric**, the last metric will be\n",
      "     |          used for early stopping.\n",
      "     |      \n",
      "     |          If early stopping occurs, the model will have three additional fields:\n",
      "     |          ``clf.best_score``, ``clf.best_iteration``.\n",
      "     |      verbose :\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation metric\n",
      "     |          measured on the validation set to stderr.\n",
      "     |      xgb_model :\n",
      "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      sample_weight_eval_set :\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
      "     |          object storing instance weights for the i-th validation set.\n",
      "     |      base_margin_eval_set :\n",
      "     |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
      "     |          object storing base margin for the i-th validation set.\n",
      "     |      feature_weights :\n",
      "     |          Weight for each feature, defines the probability of each feature being\n",
      "     |          selected when colsample is being used.  All values must be greater than 0,\n",
      "     |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n",
      "     |          `exact` tree methods.\n",
      "     |      callbacks :\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n",
      "     |                                                      save_best=True)]\n",
      "     |  \n",
      "     |  predict(self, X: Any, output_margin: bool = False, ntree_limit: Optional[int] = None, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      "     |      Predict with `X`.  If the model is trained with early stopping, then `best_iteration`\n",
      "     |      is used automatically.  For tree models, when data is on GPU, like cupy array or\n",
      "     |      cuDF dataframe and `predictor` is not specified, the prediction is run on GPU\n",
      "     |      automatically, otherwise it will run on CPU.\n",
      "     |      \n",
      "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X :\n",
      "     |          Data to predict with.\n",
      "     |      output_margin :\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit :\n",
      "     |          Deprecated, use `iteration_range` instead.\n",
      "     |      validate_features :\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are\n",
      "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      base_margin :\n",
      "     |          Margin added to prediction.\n",
      "     |      iteration_range :\n",
      "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      "     |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
      "     |          20)``, then only the forests built during [10, 20) (half open set) rounds are\n",
      "     |          used in this prediction.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction\n",
      "     |  \n",
      "     |  predict_proba(self, X: Any, ntree_limit: Optional[int] = None, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      "     |      Predict the probability of each `X` example being of a given class.\n",
      "     |      \n",
      "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix.\n",
      "     |      ntree_limit : int\n",
      "     |          Deprecated, use `iteration_range` instead.\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are\n",
      "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      base_margin : array_like\n",
      "     |          Margin added to prediction.\n",
      "     |      iteration_range :\n",
      "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      "     |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
      "     |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
      "     |          used in this prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction :\n",
      "     |          a numpy array of shape array-like of shape (n_samples, n_classes) with the\n",
      "     |          probability of each data example being of a given class.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __sklearn_is_fitted__(self) -> bool\n",
      "     |  \n",
      "     |  apply(self, X: Any, ntree_limit: int = 0, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      "     |      Return the predicted leaf every tree for each sample. If the model is trained with\n",
      "     |      early stopping, then `best_iteration` is used automatically.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      iteration_range :\n",
      "     |          See :py:meth:`xgboost.XGBRegressor.predict`.\n",
      "     |      \n",
      "     |      ntree_limit :\n",
      "     |          Deprecated, use ``iteration_range`` instead.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  get_booster(self) -> xgboost.core.Booster\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self) -> int\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self) -> Dict[str, Any]\n",
      "     |      Get xgboost specific parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n",
      "     |      Load the model from a file or bytearray. Path to file can be local\n",
      "     |      or as an URI.\n",
      "     |      \n",
      "     |      The model is loaded from XGBoost format which is universal among the various\n",
      "     |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n",
      "     |      feature_names) will not be loaded when using binary format.  To save those\n",
      "     |      attributes, use JSON instead.  See: `Model IO\n",
      "     |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      "     |      info.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname :\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal format which is universal among the\n",
      "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
      "     |      (such as feature_names) will not be saved when using binary format.  To save those\n",
      "     |      attributes, use JSON instead. See: `Model IO\n",
      "     |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      "     |      info.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or os.PathLike\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params: Any) -> 'XGBModel'\n",
      "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
      "     |      parameters that are not defined as member variables in sklearn grid\n",
      "     |      search.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from XGBModel:\n",
      "     |  \n",
      "     |  best_iteration\n",
      "     |  \n",
      "     |  best_ntree_limit\n",
      "     |  \n",
      "     |  best_score\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as\n",
      "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
      "     |          learner types, such as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property, return depends on `importance_type` parameter.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
      "     |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "    \n",
      "    class XGBModel(sklearn.base.BaseEstimator)\n",
      "     |  XGBModel(max_depth: Optional[int] = None, learning_rate: Optional[float] = None, n_estimators: int = 100, verbosity: Optional[int] = None, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = None, booster: Optional[str] = None, tree_method: Optional[str] = None, n_jobs: Optional[int] = None, gamma: Optional[float] = None, min_child_weight: Optional[float] = None, max_delta_step: Optional[float] = None, subsample: Optional[float] = None, colsample_bytree: Optional[float] = None, colsample_bylevel: Optional[float] = None, colsample_bynode: Optional[float] = None, reg_alpha: Optional[float] = None, reg_lambda: Optional[float] = None, scale_pos_weight: Optional[float] = None, base_score: Optional[float] = None, random_state: Union[numpy.random.mtrand.RandomState, int, NoneType] = None, missing: float = nan, num_parallel_tree: Optional[int] = None, monotone_constraints: Union[Dict[str, int], str, NoneType] = None, interaction_constraints: Union[str, List[Tuple[str]], NoneType] = None, importance_type: Optional[str] = None, gpu_id: Optional[int] = None, validate_parameters: Optional[bool] = None, predictor: Optional[str] = None, enable_categorical: bool = False, **kwargs: Any) -> None\n",
      "     |  \n",
      "     |  Implementation of the Scikit-Learn API for XGBoost.\n",
      "     |  \n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |      n_estimators : int\n",
      "     |          Number of gradient boosted trees.  Equivalent to number of boosting\n",
      "     |          rounds.\n",
      "     |  \n",
      "     |      max_depth :  Optional[int]\n",
      "     |          Maximum tree depth for base learners.\n",
      "     |      learning_rate : Optional[float]\n",
      "     |          Boosting learning rate (xgb's \"eta\")\n",
      "     |      verbosity : Optional[int]\n",
      "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |      objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |      booster: Optional[str]\n",
      "     |          Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |      tree_method: Optional[str]\n",
      "     |          Specify which tree method to use.  Default to auto.  If this parameter\n",
      "     |          is set to default, XGBoost will choose the most conservative option\n",
      "     |          available.  It's recommended to study this option from the parameters\n",
      "     |          document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n",
      "     |      n_jobs : Optional[int]\n",
      "     |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "     |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "     |          balance the threads.  Creating thread contention will significantly slow down both\n",
      "     |          algorithms.\n",
      "     |      gamma : Optional[float]\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf\n",
      "     |          node of the tree.\n",
      "     |      min_child_weight : Optional[float]\n",
      "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |      max_delta_step : Optional[float]\n",
      "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |      subsample : Optional[float]\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      colsample_bytree : Optional[float]\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      colsample_bylevel : Optional[float]\n",
      "     |          Subsample ratio of columns for each level.\n",
      "     |      colsample_bynode : Optional[float]\n",
      "     |          Subsample ratio of columns for each split.\n",
      "     |      reg_alpha : Optional[float]\n",
      "     |          L1 regularization term on weights (xgb's alpha).\n",
      "     |      reg_lambda : Optional[float]\n",
      "     |          L2 regularization term on weights (xgb's lambda).\n",
      "     |      scale_pos_weight : Optional[float]\n",
      "     |          Balancing of positive and negative weights.\n",
      "     |      base_score : Optional[float]\n",
      "     |          The initial prediction score of all instances, global bias.\n",
      "     |      random_state : Optional[Union[numpy.random.RandomState, int]]\n",
      "     |          Random number seed.\n",
      "     |  \n",
      "     |          .. note::\n",
      "     |  \n",
      "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      "     |             it uses Hogwild algorithm.\n",
      "     |  \n",
      "     |      missing : float, default np.nan\n",
      "     |          Value in the data which needs to be present as a missing value.\n",
      "     |      num_parallel_tree: Optional[int]\n",
      "     |          Used for boosting random forest.\n",
      "     |      monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
      "     |          Constraint of variable monotonicity.  See tutorial for more\n",
      "     |          information.\n",
      "     |      interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
      "     |          Constraints for interaction representing permitted interactions.  The\n",
      "     |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "     |          [2, 3, 4]], where each inner list is a group of indices of features\n",
      "     |          that are allowed to interact with each other.  See tutorial for more\n",
      "     |          information\n",
      "     |      importance_type: Optional[str]\n",
      "     |          The feature importance type for the feature_importances\\_ property:\n",
      "     |  \n",
      "     |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      "     |            \"total_cover\".\n",
      "     |          * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
      "     |            without bias.\n",
      "     |  \n",
      "     |      gpu_id : Optional[int]\n",
      "     |          Device ordinal.\n",
      "     |      validate_parameters : Optional[bool]\n",
      "     |          Give warnings for unknown parameter.\n",
      "     |      predictor : Optional[str]\n",
      "     |          Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n",
      "     |          gpu_predictor].\n",
      "     |      enable_categorical : bool\n",
      "     |  \n",
      "     |          .. versionadded:: 1.5.0\n",
      "     |  \n",
      "     |          Experimental support for categorical data.  Do not set to true unless you are\n",
      "     |          interested in development. Only valid when `gpu_hist` and dataframe are used.\n",
      "     |  \n",
      "     |      kwargs : dict, optional\n",
      "     |          Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "     |          parameters can be found here:\n",
      "     |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "     |          dict simultaneously will result in a TypeError.\n",
      "     |  \n",
      "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "     |              that parameters passed via this argument will interact properly\n",
      "     |              with scikit-learn.\n",
      "     |  \n",
      "     |          .. note::  Custom objective function\n",
      "     |  \n",
      "     |              A custom objective function can be provided for the ``objective``\n",
      "     |              parameter. In this case, it should have the signature\n",
      "     |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |              y_true: array_like of shape [n_samples]\n",
      "     |                  The target values\n",
      "     |              y_pred: array_like of shape [n_samples]\n",
      "     |                  The predicted values\n",
      "     |  \n",
      "     |              grad: array_like of shape [n_samples]\n",
      "     |                  The value of the gradient for each sample point.\n",
      "     |              hess: array_like of shape [n_samples]\n",
      "     |                  The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth: Optional[int] = None, learning_rate: Optional[float] = None, n_estimators: int = 100, verbosity: Optional[int] = None, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = None, booster: Optional[str] = None, tree_method: Optional[str] = None, n_jobs: Optional[int] = None, gamma: Optional[float] = None, min_child_weight: Optional[float] = None, max_delta_step: Optional[float] = None, subsample: Optional[float] = None, colsample_bytree: Optional[float] = None, colsample_bylevel: Optional[float] = None, colsample_bynode: Optional[float] = None, reg_alpha: Optional[float] = None, reg_lambda: Optional[float] = None, scale_pos_weight: Optional[float] = None, base_score: Optional[float] = None, random_state: Union[numpy.random.mtrand.RandomState, int, NoneType] = None, missing: float = nan, num_parallel_tree: Optional[int] = None, monotone_constraints: Union[Dict[str, int], str, NoneType] = None, interaction_constraints: Union[str, List[Tuple[str]], NoneType] = None, importance_type: Optional[str] = None, gpu_id: Optional[int] = None, validate_parameters: Optional[bool] = None, predictor: Optional[str] = None, enable_categorical: bool = False, **kwargs: Any) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_is_fitted__(self) -> bool\n",
      "     |  \n",
      "     |  apply(self, X: Any, ntree_limit: int = 0, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      "     |      Return the predicted leaf every tree for each sample. If the model is trained with\n",
      "     |      early stopping, then `best_iteration` is used automatically.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      iteration_range :\n",
      "     |          See :py:meth:`xgboost.XGBRegressor.predict`.\n",
      "     |      \n",
      "     |      ntree_limit :\n",
      "     |          Deprecated, use ``iteration_range`` instead.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self) -> Dict[str, Dict[str, Union[List[float], List[Tuple[float, float]]]]]\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[List[Tuple[Any, Any]]] = None, eval_metric: Union[str, List[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]], NoneType] = None, early_stopping_rounds: Optional[int] = None, verbose: Optional[bool] = True, xgb_model: Union[xgboost.core.Booster, ForwardRef('XGBModel'), str, NoneType] = None, sample_weight_eval_set: Optional[List[Any]] = None, base_margin_eval_set: Optional[List[Any]] = None, feature_weights: Optional[Any] = None, callbacks: Optional[List[xgboost.callback.TrainingCallback]] = None) -> 'XGBModel'\n",
      "     |      Fit gradient boosting model.\n",
      "     |      \n",
      "     |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
      "     |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
      "     |      pass ``xgb_model`` argument.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X :\n",
      "     |          Feature matrix\n",
      "     |      y :\n",
      "     |          Labels\n",
      "     |      sample_weight :\n",
      "     |          instance weights\n",
      "     |      base_margin :\n",
      "     |          global bias for each instance.\n",
      "     |      eval_set :\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      "     |          metrics will be computed.\n",
      "     |          Validation metrics will help us track the performance of the model.\n",
      "     |      eval_metric :\n",
      "     |          If a str, should be a built-in evaluation metric to use. See doc/parameter.rst.\n",
      "     |      \n",
      "     |          If a list of str, should be the list of multiple built-in evaluation metrics\n",
      "     |          to use.\n",
      "     |      \n",
      "     |          If callable, a custom evaluation metric. The call signature is\n",
      "     |          ``func(y_predicted, y_true)`` where ``y_true`` will be a DMatrix object such\n",
      "     |          that you may need to call the ``get_label`` method. It must return a str,\n",
      "     |          value pair where the str is a name for the evaluation and value is the value\n",
      "     |          of the evaluation function. The callable custom objective is always minimized.\n",
      "     |      early_stopping_rounds :\n",
      "     |          Activates early stopping. Validation metric needs to improve at least once in\n",
      "     |          every **early_stopping_rounds** round(s) to continue training.\n",
      "     |          Requires at least one item in **eval_set**.\n",
      "     |      \n",
      "     |          The method returns the model from the last iteration (not the best one).\n",
      "     |          If there's more than one item in **eval_set**, the last entry will be used\n",
      "     |          for early stopping.\n",
      "     |      \n",
      "     |          If there's more than one metric in **eval_metric**, the last metric will be\n",
      "     |          used for early stopping.\n",
      "     |      \n",
      "     |          If early stopping occurs, the model will have three additional fields:\n",
      "     |          ``clf.best_score``, ``clf.best_iteration``.\n",
      "     |      verbose :\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation metric\n",
      "     |          measured on the validation set to stderr.\n",
      "     |      xgb_model :\n",
      "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      sample_weight_eval_set :\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
      "     |          object storing instance weights for the i-th validation set.\n",
      "     |      base_margin_eval_set :\n",
      "     |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
      "     |          object storing base margin for the i-th validation set.\n",
      "     |      feature_weights :\n",
      "     |          Weight for each feature, defines the probability of each feature being\n",
      "     |          selected when colsample is being used.  All values must be greater than 0,\n",
      "     |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n",
      "     |          `exact` tree methods.\n",
      "     |      callbacks :\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n",
      "     |                                                      save_best=True)]\n",
      "     |  \n",
      "     |  get_booster(self) -> xgboost.core.Booster\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self) -> int\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self) -> Dict[str, Any]\n",
      "     |      Get xgboost specific parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n",
      "     |      Load the model from a file or bytearray. Path to file can be local\n",
      "     |      or as an URI.\n",
      "     |      \n",
      "     |      The model is loaded from XGBoost format which is universal among the various\n",
      "     |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n",
      "     |      feature_names) will not be loaded when using binary format.  To save those\n",
      "     |      attributes, use JSON instead.  See: `Model IO\n",
      "     |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      "     |      info.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname :\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  predict(self, X: Any, output_margin: bool = False, ntree_limit: Optional[int] = None, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      "     |      Predict with `X`.  If the model is trained with early stopping, then `best_iteration`\n",
      "     |      is used automatically.  For tree models, when data is on GPU, like cupy array or\n",
      "     |      cuDF dataframe and `predictor` is not specified, the prediction is run on GPU\n",
      "     |      automatically, otherwise it will run on CPU.\n",
      "     |      \n",
      "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X :\n",
      "     |          Data to predict with.\n",
      "     |      output_margin :\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit :\n",
      "     |          Deprecated, use `iteration_range` instead.\n",
      "     |      validate_features :\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are\n",
      "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      base_margin :\n",
      "     |          Margin added to prediction.\n",
      "     |      iteration_range :\n",
      "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      "     |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
      "     |          20)``, then only the forests built during [10, 20) (half open set) rounds are\n",
      "     |          used in this prediction.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction\n",
      "     |  \n",
      "     |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal format which is universal among the\n",
      "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
      "     |      (such as feature_names) will not be saved when using binary format.  To save those\n",
      "     |      attributes, use JSON instead. See: `Model IO\n",
      "     |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      "     |      info.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or os.PathLike\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params: Any) -> 'XGBModel'\n",
      "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
      "     |      parameters that are not defined as member variables in sklearn grid\n",
      "     |      search.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  best_iteration\n",
      "     |  \n",
      "     |  best_ntree_limit\n",
      "     |  \n",
      "     |  best_score\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as\n",
      "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
      "     |          learner types, such as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property, return depends on `importance_type` parameter.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
      "     |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class XGBRFClassifier(XGBClassifier)\n",
      "     |  XGBRFClassifier(*, learning_rate: float = 1.0, subsample: float = 0.8, colsample_bynode: float = 0.8, reg_lambda: float = 1e-05, use_label_encoder: bool = True, **kwargs: Any)\n",
      "     |  \n",
      "     |  scikit-learn API for XGBoost random forest classification.\n",
      "     |  \n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |      n_estimators : int\n",
      "     |          Number of trees in random forest to fit.\n",
      "     |      use_label_encoder : bool\n",
      "     |          (Deprecated) Use the label encoder from scikit-learn to encode the labels. For new\n",
      "     |          code, we recommend that you set this parameter to False.\n",
      "     |  \n",
      "     |      max_depth :  Optional[int]\n",
      "     |          Maximum tree depth for base learners.\n",
      "     |      learning_rate : Optional[float]\n",
      "     |          Boosting learning rate (xgb's \"eta\")\n",
      "     |      verbosity : Optional[int]\n",
      "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |      objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |      booster: Optional[str]\n",
      "     |          Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |      tree_method: Optional[str]\n",
      "     |          Specify which tree method to use.  Default to auto.  If this parameter\n",
      "     |          is set to default, XGBoost will choose the most conservative option\n",
      "     |          available.  It's recommended to study this option from the parameters\n",
      "     |          document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n",
      "     |      n_jobs : Optional[int]\n",
      "     |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "     |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "     |          balance the threads.  Creating thread contention will significantly slow down both\n",
      "     |          algorithms.\n",
      "     |      gamma : Optional[float]\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf\n",
      "     |          node of the tree.\n",
      "     |      min_child_weight : Optional[float]\n",
      "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |      max_delta_step : Optional[float]\n",
      "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |      subsample : Optional[float]\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      colsample_bytree : Optional[float]\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      colsample_bylevel : Optional[float]\n",
      "     |          Subsample ratio of columns for each level.\n",
      "     |      colsample_bynode : Optional[float]\n",
      "     |          Subsample ratio of columns for each split.\n",
      "     |      reg_alpha : Optional[float]\n",
      "     |          L1 regularization term on weights (xgb's alpha).\n",
      "     |      reg_lambda : Optional[float]\n",
      "     |          L2 regularization term on weights (xgb's lambda).\n",
      "     |      scale_pos_weight : Optional[float]\n",
      "     |          Balancing of positive and negative weights.\n",
      "     |      base_score : Optional[float]\n",
      "     |          The initial prediction score of all instances, global bias.\n",
      "     |      random_state : Optional[Union[numpy.random.RandomState, int]]\n",
      "     |          Random number seed.\n",
      "     |  \n",
      "     |          .. note::\n",
      "     |  \n",
      "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      "     |             it uses Hogwild algorithm.\n",
      "     |  \n",
      "     |      missing : float, default np.nan\n",
      "     |          Value in the data which needs to be present as a missing value.\n",
      "     |      num_parallel_tree: Optional[int]\n",
      "     |          Used for boosting random forest.\n",
      "     |      monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
      "     |          Constraint of variable monotonicity.  See tutorial for more\n",
      "     |          information.\n",
      "     |      interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
      "     |          Constraints for interaction representing permitted interactions.  The\n",
      "     |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "     |          [2, 3, 4]], where each inner list is a group of indices of features\n",
      "     |          that are allowed to interact with each other.  See tutorial for more\n",
      "     |          information\n",
      "     |      importance_type: Optional[str]\n",
      "     |          The feature importance type for the feature_importances\\_ property:\n",
      "     |  \n",
      "     |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      "     |            \"total_cover\".\n",
      "     |          * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
      "     |            without bias.\n",
      "     |  \n",
      "     |      gpu_id : Optional[int]\n",
      "     |          Device ordinal.\n",
      "     |      validate_parameters : Optional[bool]\n",
      "     |          Give warnings for unknown parameter.\n",
      "     |      predictor : Optional[str]\n",
      "     |          Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n",
      "     |          gpu_predictor].\n",
      "     |      enable_categorical : bool\n",
      "     |  \n",
      "     |          .. versionadded:: 1.5.0\n",
      "     |  \n",
      "     |          Experimental support for categorical data.  Do not set to true unless you are\n",
      "     |          interested in development. Only valid when `gpu_hist` and dataframe are used.\n",
      "     |  \n",
      "     |      kwargs : dict, optional\n",
      "     |          Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "     |          parameters can be found here:\n",
      "     |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "     |          dict simultaneously will result in a TypeError.\n",
      "     |  \n",
      "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "     |              that parameters passed via this argument will interact properly\n",
      "     |              with scikit-learn.\n",
      "     |  \n",
      "     |          .. note::  Custom objective function\n",
      "     |  \n",
      "     |              A custom objective function can be provided for the ``objective``\n",
      "     |              parameter. In this case, it should have the signature\n",
      "     |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |              y_true: array_like of shape [n_samples]\n",
      "     |                  The target values\n",
      "     |              y_pred: array_like of shape [n_samples]\n",
      "     |                  The predicted values\n",
      "     |  \n",
      "     |              grad: array_like of shape [n_samples]\n",
      "     |                  The value of the gradient for each sample point.\n",
      "     |              hess: array_like of shape [n_samples]\n",
      "     |                  The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRFClassifier\n",
      "     |      XGBClassifier\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, learning_rate: float = 1.0, subsample: float = 0.8, colsample_bynode: float = 0.8, reg_lambda: float = 1e-05, use_label_encoder: bool = True, **kwargs: Any)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[List[Tuple[Any, Any]]] = None, eval_metric: Union[str, List[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]], NoneType] = None, early_stopping_rounds: Optional[int] = None, verbose: Optional[bool] = True, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Optional[List[Any]] = None, base_margin_eval_set: Optional[List[Any]] = None, feature_weights: Optional[Any] = None, callbacks: Optional[List[xgboost.callback.TrainingCallback]] = None) -> 'XGBRFClassifier'\n",
      "     |      Fit gradient boosting classifier.\n",
      "     |      \n",
      "     |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
      "     |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
      "     |      pass ``xgb_model`` argument.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X :\n",
      "     |          Feature matrix\n",
      "     |      y :\n",
      "     |          Labels\n",
      "     |      sample_weight :\n",
      "     |          instance weights\n",
      "     |      base_margin :\n",
      "     |          global bias for each instance.\n",
      "     |      eval_set :\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      "     |          metrics will be computed.\n",
      "     |          Validation metrics will help us track the performance of the model.\n",
      "     |      eval_metric :\n",
      "     |          If a str, should be a built-in evaluation metric to use. See doc/parameter.rst.\n",
      "     |      \n",
      "     |          If a list of str, should be the list of multiple built-in evaluation metrics\n",
      "     |          to use.\n",
      "     |      \n",
      "     |          If callable, a custom evaluation metric. The call signature is\n",
      "     |          ``func(y_predicted, y_true)`` where ``y_true`` will be a DMatrix object such\n",
      "     |          that you may need to call the ``get_label`` method. It must return a str,\n",
      "     |          value pair where the str is a name for the evaluation and value is the value\n",
      "     |          of the evaluation function. The callable custom objective is always minimized.\n",
      "     |      early_stopping_rounds :\n",
      "     |          Activates early stopping. Validation metric needs to improve at least once in\n",
      "     |          every **early_stopping_rounds** round(s) to continue training.\n",
      "     |          Requires at least one item in **eval_set**.\n",
      "     |      \n",
      "     |          The method returns the model from the last iteration (not the best one).\n",
      "     |          If there's more than one item in **eval_set**, the last entry will be used\n",
      "     |          for early stopping.\n",
      "     |      \n",
      "     |          If there's more than one metric in **eval_metric**, the last metric will be\n",
      "     |          used for early stopping.\n",
      "     |      \n",
      "     |          If early stopping occurs, the model will have three additional fields:\n",
      "     |          ``clf.best_score``, ``clf.best_iteration``.\n",
      "     |      verbose :\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation metric\n",
      "     |          measured on the validation set to stderr.\n",
      "     |      xgb_model :\n",
      "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      sample_weight_eval_set :\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
      "     |          object storing instance weights for the i-th validation set.\n",
      "     |      base_margin_eval_set :\n",
      "     |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
      "     |          object storing base margin for the i-th validation set.\n",
      "     |      feature_weights :\n",
      "     |          Weight for each feature, defines the probability of each feature being\n",
      "     |          selected when colsample is being used.  All values must be greater than 0,\n",
      "     |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n",
      "     |          `exact` tree methods.\n",
      "     |      callbacks :\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n",
      "     |                                                      save_best=True)]\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self) -> int\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_xgb_params(self) -> Dict[str, Any]\n",
      "     |      Get xgboost specific parameters.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBClassifier:\n",
      "     |  \n",
      "     |  evals_result(self) -> Dict[str, Dict[str, Union[List[float], List[Tuple[float, float]]]]]\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBClassifier(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  predict(self, X: Any, output_margin: bool = False, ntree_limit: Optional[int] = None, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      "     |      Predict with `X`.  If the model is trained with early stopping, then `best_iteration`\n",
      "     |      is used automatically.  For tree models, when data is on GPU, like cupy array or\n",
      "     |      cuDF dataframe and `predictor` is not specified, the prediction is run on GPU\n",
      "     |      automatically, otherwise it will run on CPU.\n",
      "     |      \n",
      "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X :\n",
      "     |          Data to predict with.\n",
      "     |      output_margin :\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit :\n",
      "     |          Deprecated, use `iteration_range` instead.\n",
      "     |      validate_features :\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are\n",
      "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      base_margin :\n",
      "     |          Margin added to prediction.\n",
      "     |      iteration_range :\n",
      "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      "     |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
      "     |          20)``, then only the forests built during [10, 20) (half open set) rounds are\n",
      "     |          used in this prediction.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction\n",
      "     |  \n",
      "     |  predict_proba(self, X: Any, ntree_limit: Optional[int] = None, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      "     |      Predict the probability of each `X` example being of a given class.\n",
      "     |      \n",
      "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix.\n",
      "     |      ntree_limit : int\n",
      "     |          Deprecated, use `iteration_range` instead.\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are\n",
      "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      base_margin : array_like\n",
      "     |          Margin added to prediction.\n",
      "     |      iteration_range :\n",
      "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      "     |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
      "     |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
      "     |          used in this prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction :\n",
      "     |          a numpy array of shape array-like of shape (n_samples, n_classes) with the\n",
      "     |          probability of each data example being of a given class.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __sklearn_is_fitted__(self) -> bool\n",
      "     |  \n",
      "     |  apply(self, X: Any, ntree_limit: int = 0, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      "     |      Return the predicted leaf every tree for each sample. If the model is trained with\n",
      "     |      early stopping, then `best_iteration` is used automatically.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      iteration_range :\n",
      "     |          See :py:meth:`xgboost.XGBRegressor.predict`.\n",
      "     |      \n",
      "     |      ntree_limit :\n",
      "     |          Deprecated, use ``iteration_range`` instead.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  get_booster(self) -> xgboost.core.Booster\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n",
      "     |      Load the model from a file or bytearray. Path to file can be local\n",
      "     |      or as an URI.\n",
      "     |      \n",
      "     |      The model is loaded from XGBoost format which is universal among the various\n",
      "     |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n",
      "     |      feature_names) will not be loaded when using binary format.  To save those\n",
      "     |      attributes, use JSON instead.  See: `Model IO\n",
      "     |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      "     |      info.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname :\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal format which is universal among the\n",
      "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
      "     |      (such as feature_names) will not be saved when using binary format.  To save those\n",
      "     |      attributes, use JSON instead. See: `Model IO\n",
      "     |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      "     |      info.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or os.PathLike\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params: Any) -> 'XGBModel'\n",
      "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
      "     |      parameters that are not defined as member variables in sklearn grid\n",
      "     |      search.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from XGBModel:\n",
      "     |  \n",
      "     |  best_iteration\n",
      "     |  \n",
      "     |  best_ntree_limit\n",
      "     |  \n",
      "     |  best_score\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as\n",
      "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
      "     |          learner types, such as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property, return depends on `importance_type` parameter.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
      "     |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "    \n",
      "    class XGBRFRegressor(XGBRegressor)\n",
      "     |  XGBRFRegressor(*, learning_rate: float = 1.0, subsample: float = 0.8, colsample_bynode: float = 0.8, reg_lambda: float = 1e-05, **kwargs: Any) -> None\n",
      "     |  \n",
      "     |  scikit-learn API for XGBoost random forest regression.\n",
      "     |  \n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |      n_estimators : int\n",
      "     |          Number of trees in random forest to fit.\n",
      "     |  \n",
      "     |      max_depth :  Optional[int]\n",
      "     |          Maximum tree depth for base learners.\n",
      "     |      learning_rate : Optional[float]\n",
      "     |          Boosting learning rate (xgb's \"eta\")\n",
      "     |      verbosity : Optional[int]\n",
      "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |      objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |      booster: Optional[str]\n",
      "     |          Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |      tree_method: Optional[str]\n",
      "     |          Specify which tree method to use.  Default to auto.  If this parameter\n",
      "     |          is set to default, XGBoost will choose the most conservative option\n",
      "     |          available.  It's recommended to study this option from the parameters\n",
      "     |          document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n",
      "     |      n_jobs : Optional[int]\n",
      "     |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "     |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "     |          balance the threads.  Creating thread contention will significantly slow down both\n",
      "     |          algorithms.\n",
      "     |      gamma : Optional[float]\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf\n",
      "     |          node of the tree.\n",
      "     |      min_child_weight : Optional[float]\n",
      "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |      max_delta_step : Optional[float]\n",
      "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |      subsample : Optional[float]\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      colsample_bytree : Optional[float]\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      colsample_bylevel : Optional[float]\n",
      "     |          Subsample ratio of columns for each level.\n",
      "     |      colsample_bynode : Optional[float]\n",
      "     |          Subsample ratio of columns for each split.\n",
      "     |      reg_alpha : Optional[float]\n",
      "     |          L1 regularization term on weights (xgb's alpha).\n",
      "     |      reg_lambda : Optional[float]\n",
      "     |          L2 regularization term on weights (xgb's lambda).\n",
      "     |      scale_pos_weight : Optional[float]\n",
      "     |          Balancing of positive and negative weights.\n",
      "     |      base_score : Optional[float]\n",
      "     |          The initial prediction score of all instances, global bias.\n",
      "     |      random_state : Optional[Union[numpy.random.RandomState, int]]\n",
      "     |          Random number seed.\n",
      "     |  \n",
      "     |          .. note::\n",
      "     |  \n",
      "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      "     |             it uses Hogwild algorithm.\n",
      "     |  \n",
      "     |      missing : float, default np.nan\n",
      "     |          Value in the data which needs to be present as a missing value.\n",
      "     |      num_parallel_tree: Optional[int]\n",
      "     |          Used for boosting random forest.\n",
      "     |      monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
      "     |          Constraint of variable monotonicity.  See tutorial for more\n",
      "     |          information.\n",
      "     |      interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
      "     |          Constraints for interaction representing permitted interactions.  The\n",
      "     |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "     |          [2, 3, 4]], where each inner list is a group of indices of features\n",
      "     |          that are allowed to interact with each other.  See tutorial for more\n",
      "     |          information\n",
      "     |      importance_type: Optional[str]\n",
      "     |          The feature importance type for the feature_importances\\_ property:\n",
      "     |  \n",
      "     |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      "     |            \"total_cover\".\n",
      "     |          * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
      "     |            without bias.\n",
      "     |  \n",
      "     |      gpu_id : Optional[int]\n",
      "     |          Device ordinal.\n",
      "     |      validate_parameters : Optional[bool]\n",
      "     |          Give warnings for unknown parameter.\n",
      "     |      predictor : Optional[str]\n",
      "     |          Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n",
      "     |          gpu_predictor].\n",
      "     |      enable_categorical : bool\n",
      "     |  \n",
      "     |          .. versionadded:: 1.5.0\n",
      "     |  \n",
      "     |          Experimental support for categorical data.  Do not set to true unless you are\n",
      "     |          interested in development. Only valid when `gpu_hist` and dataframe are used.\n",
      "     |  \n",
      "     |      kwargs : dict, optional\n",
      "     |          Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "     |          parameters can be found here:\n",
      "     |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "     |          dict simultaneously will result in a TypeError.\n",
      "     |  \n",
      "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "     |              that parameters passed via this argument will interact properly\n",
      "     |              with scikit-learn.\n",
      "     |  \n",
      "     |          .. note::  Custom objective function\n",
      "     |  \n",
      "     |              A custom objective function can be provided for the ``objective``\n",
      "     |              parameter. In this case, it should have the signature\n",
      "     |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |              y_true: array_like of shape [n_samples]\n",
      "     |                  The target values\n",
      "     |              y_pred: array_like of shape [n_samples]\n",
      "     |                  The predicted values\n",
      "     |  \n",
      "     |              grad: array_like of shape [n_samples]\n",
      "     |                  The value of the gradient for each sample point.\n",
      "     |              hess: array_like of shape [n_samples]\n",
      "     |                  The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRFRegressor\n",
      "     |      XGBRegressor\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, learning_rate: float = 1.0, subsample: float = 0.8, colsample_bynode: float = 0.8, reg_lambda: float = 1e-05, **kwargs: Any) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[List[Tuple[Any, Any]]] = None, eval_metric: Union[str, List[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]], NoneType] = None, early_stopping_rounds: Optional[int] = None, verbose: Optional[bool] = True, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Optional[List[Any]] = None, base_margin_eval_set: Optional[List[Any]] = None, feature_weights: Optional[Any] = None, callbacks: Optional[List[xgboost.callback.TrainingCallback]] = None) -> 'XGBRFRegressor'\n",
      "     |      Fit gradient boosting model.\n",
      "     |      \n",
      "     |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
      "     |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
      "     |      pass ``xgb_model`` argument.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X :\n",
      "     |          Feature matrix\n",
      "     |      y :\n",
      "     |          Labels\n",
      "     |      sample_weight :\n",
      "     |          instance weights\n",
      "     |      base_margin :\n",
      "     |          global bias for each instance.\n",
      "     |      eval_set :\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      "     |          metrics will be computed.\n",
      "     |          Validation metrics will help us track the performance of the model.\n",
      "     |      eval_metric :\n",
      "     |          If a str, should be a built-in evaluation metric to use. See doc/parameter.rst.\n",
      "     |      \n",
      "     |          If a list of str, should be the list of multiple built-in evaluation metrics\n",
      "     |          to use.\n",
      "     |      \n",
      "     |          If callable, a custom evaluation metric. The call signature is\n",
      "     |          ``func(y_predicted, y_true)`` where ``y_true`` will be a DMatrix object such\n",
      "     |          that you may need to call the ``get_label`` method. It must return a str,\n",
      "     |          value pair where the str is a name for the evaluation and value is the value\n",
      "     |          of the evaluation function. The callable custom objective is always minimized.\n",
      "     |      early_stopping_rounds :\n",
      "     |          Activates early stopping. Validation metric needs to improve at least once in\n",
      "     |          every **early_stopping_rounds** round(s) to continue training.\n",
      "     |          Requires at least one item in **eval_set**.\n",
      "     |      \n",
      "     |          The method returns the model from the last iteration (not the best one).\n",
      "     |          If there's more than one item in **eval_set**, the last entry will be used\n",
      "     |          for early stopping.\n",
      "     |      \n",
      "     |          If there's more than one metric in **eval_metric**, the last metric will be\n",
      "     |          used for early stopping.\n",
      "     |      \n",
      "     |          If early stopping occurs, the model will have three additional fields:\n",
      "     |          ``clf.best_score``, ``clf.best_iteration``.\n",
      "     |      verbose :\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation metric\n",
      "     |          measured on the validation set to stderr.\n",
      "     |      xgb_model :\n",
      "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      sample_weight_eval_set :\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
      "     |          object storing instance weights for the i-th validation set.\n",
      "     |      base_margin_eval_set :\n",
      "     |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
      "     |          object storing base margin for the i-th validation set.\n",
      "     |      feature_weights :\n",
      "     |          Weight for each feature, defines the probability of each feature being\n",
      "     |          selected when colsample is being used.  All values must be greater than 0,\n",
      "     |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n",
      "     |          `exact` tree methods.\n",
      "     |      callbacks :\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n",
      "     |                                                      save_best=True)]\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self) -> int\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_xgb_params(self) -> Dict[str, Any]\n",
      "     |      Get xgboost specific parameters.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __sklearn_is_fitted__(self) -> bool\n",
      "     |  \n",
      "     |  apply(self, X: Any, ntree_limit: int = 0, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      "     |      Return the predicted leaf every tree for each sample. If the model is trained with\n",
      "     |      early stopping, then `best_iteration` is used automatically.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      iteration_range :\n",
      "     |          See :py:meth:`xgboost.XGBRegressor.predict`.\n",
      "     |      \n",
      "     |      ntree_limit :\n",
      "     |          Deprecated, use ``iteration_range`` instead.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self) -> Dict[str, Dict[str, Union[List[float], List[Tuple[float, float]]]]]\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  get_booster(self) -> xgboost.core.Booster\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n",
      "     |      Load the model from a file or bytearray. Path to file can be local\n",
      "     |      or as an URI.\n",
      "     |      \n",
      "     |      The model is loaded from XGBoost format which is universal among the various\n",
      "     |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n",
      "     |      feature_names) will not be loaded when using binary format.  To save those\n",
      "     |      attributes, use JSON instead.  See: `Model IO\n",
      "     |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      "     |      info.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname :\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  predict(self, X: Any, output_margin: bool = False, ntree_limit: Optional[int] = None, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      "     |      Predict with `X`.  If the model is trained with early stopping, then `best_iteration`\n",
      "     |      is used automatically.  For tree models, when data is on GPU, like cupy array or\n",
      "     |      cuDF dataframe and `predictor` is not specified, the prediction is run on GPU\n",
      "     |      automatically, otherwise it will run on CPU.\n",
      "     |      \n",
      "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X :\n",
      "     |          Data to predict with.\n",
      "     |      output_margin :\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit :\n",
      "     |          Deprecated, use `iteration_range` instead.\n",
      "     |      validate_features :\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are\n",
      "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      base_margin :\n",
      "     |          Margin added to prediction.\n",
      "     |      iteration_range :\n",
      "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      "     |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
      "     |          20)``, then only the forests built during [10, 20) (half open set) rounds are\n",
      "     |          used in this prediction.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction\n",
      "     |  \n",
      "     |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal format which is universal among the\n",
      "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
      "     |      (such as feature_names) will not be saved when using binary format.  To save those\n",
      "     |      attributes, use JSON instead. See: `Model IO\n",
      "     |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      "     |      info.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or os.PathLike\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params: Any) -> 'XGBModel'\n",
      "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
      "     |      parameters that are not defined as member variables in sklearn grid\n",
      "     |      search.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from XGBModel:\n",
      "     |  \n",
      "     |  best_iteration\n",
      "     |  \n",
      "     |  best_ntree_limit\n",
      "     |  \n",
      "     |  best_score\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as\n",
      "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
      "     |          learner types, such as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property, return depends on `importance_type` parameter.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
      "     |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "    \n",
      "    class XGBRanker(XGBModel, XGBRankerMixIn)\n",
      "     |  XGBRanker(*, objective: str = 'rank:pairwise', **kwargs: Any)\n",
      "     |  \n",
      "     |  Implementation of the Scikit-Learn API for XGBoost Ranking.\n",
      "     |  \n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |      n_estimators : int\n",
      "     |          Number of gradient boosted trees.  Equivalent to number of boosting\n",
      "     |          rounds.\n",
      "     |  \n",
      "     |      max_depth :  Optional[int]\n",
      "     |          Maximum tree depth for base learners.\n",
      "     |      learning_rate : Optional[float]\n",
      "     |          Boosting learning rate (xgb's \"eta\")\n",
      "     |      verbosity : Optional[int]\n",
      "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |      objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |      booster: Optional[str]\n",
      "     |          Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |      tree_method: Optional[str]\n",
      "     |          Specify which tree method to use.  Default to auto.  If this parameter\n",
      "     |          is set to default, XGBoost will choose the most conservative option\n",
      "     |          available.  It's recommended to study this option from the parameters\n",
      "     |          document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n",
      "     |      n_jobs : Optional[int]\n",
      "     |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "     |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "     |          balance the threads.  Creating thread contention will significantly slow down both\n",
      "     |          algorithms.\n",
      "     |      gamma : Optional[float]\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf\n",
      "     |          node of the tree.\n",
      "     |      min_child_weight : Optional[float]\n",
      "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |      max_delta_step : Optional[float]\n",
      "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |      subsample : Optional[float]\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      colsample_bytree : Optional[float]\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      colsample_bylevel : Optional[float]\n",
      "     |          Subsample ratio of columns for each level.\n",
      "     |      colsample_bynode : Optional[float]\n",
      "     |          Subsample ratio of columns for each split.\n",
      "     |      reg_alpha : Optional[float]\n",
      "     |          L1 regularization term on weights (xgb's alpha).\n",
      "     |      reg_lambda : Optional[float]\n",
      "     |          L2 regularization term on weights (xgb's lambda).\n",
      "     |      scale_pos_weight : Optional[float]\n",
      "     |          Balancing of positive and negative weights.\n",
      "     |      base_score : Optional[float]\n",
      "     |          The initial prediction score of all instances, global bias.\n",
      "     |      random_state : Optional[Union[numpy.random.RandomState, int]]\n",
      "     |          Random number seed.\n",
      "     |  \n",
      "     |          .. note::\n",
      "     |  \n",
      "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      "     |             it uses Hogwild algorithm.\n",
      "     |  \n",
      "     |      missing : float, default np.nan\n",
      "     |          Value in the data which needs to be present as a missing value.\n",
      "     |      num_parallel_tree: Optional[int]\n",
      "     |          Used for boosting random forest.\n",
      "     |      monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
      "     |          Constraint of variable monotonicity.  See tutorial for more\n",
      "     |          information.\n",
      "     |      interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
      "     |          Constraints for interaction representing permitted interactions.  The\n",
      "     |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "     |          [2, 3, 4]], where each inner list is a group of indices of features\n",
      "     |          that are allowed to interact with each other.  See tutorial for more\n",
      "     |          information\n",
      "     |      importance_type: Optional[str]\n",
      "     |          The feature importance type for the feature_importances\\_ property:\n",
      "     |  \n",
      "     |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      "     |            \"total_cover\".\n",
      "     |          * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
      "     |            without bias.\n",
      "     |  \n",
      "     |      gpu_id : Optional[int]\n",
      "     |          Device ordinal.\n",
      "     |      validate_parameters : Optional[bool]\n",
      "     |          Give warnings for unknown parameter.\n",
      "     |      predictor : Optional[str]\n",
      "     |          Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n",
      "     |          gpu_predictor].\n",
      "     |      enable_categorical : bool\n",
      "     |  \n",
      "     |          .. versionadded:: 1.5.0\n",
      "     |  \n",
      "     |          Experimental support for categorical data.  Do not set to true unless you are\n",
      "     |          interested in development. Only valid when `gpu_hist` and dataframe are used.\n",
      "     |  \n",
      "     |      kwargs : dict, optional\n",
      "     |          Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "     |          parameters can be found here:\n",
      "     |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "     |          dict simultaneously will result in a TypeError.\n",
      "     |  \n",
      "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "     |              that parameters passed via this argument will interact properly\n",
      "     |              with scikit-learn.\n",
      "     |  \n",
      "     |          Note\n",
      "     |          ----\n",
      "     |          A custom objective function is currently not supported by XGBRanker.\n",
      "     |          Likewise, a custom metric function is not supported either.\n",
      "     |  \n",
      "     |          Note\n",
      "     |          ----\n",
      "     |          Query group information is required for ranking tasks by either using the `group`\n",
      "     |          parameter or `qid` parameter in `fit` method.\n",
      "     |  \n",
      "     |          Before fitting the model, your data need to be sorted by query group. When fitting\n",
      "     |          the model, you need to provide an additional array that contains the size of each\n",
      "     |          query group.\n",
      "     |  \n",
      "     |          For example, if your original data look like:\n",
      "     |  \n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   qid |   label   |   features    |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   1   |   0       |   x_1         |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   1   |   1       |   x_2         |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   1   |   0       |   x_3         |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   2   |   0       |   x_4         |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   2   |   1       |   x_5         |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   2   |   1       |   x_6         |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |          |   2   |   1       |   x_7         |\n",
      "     |          +-------+-----------+---------------+\n",
      "     |  \n",
      "     |          then your group array should be ``[3, 4]``.  Sometimes using query id (`qid`)\n",
      "     |          instead of group can be more convenient.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRanker\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      XGBRankerMixIn\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, objective: str = 'rank:pairwise', **kwargs: Any)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X: Any, y: Any, *, group: Optional[Any] = None, qid: Optional[Any] = None, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[List[Tuple[Any, Any]]] = None, eval_group: Optional[List[Any]] = None, eval_qid: Optional[List[Any]] = None, eval_metric: Union[str, List[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]], NoneType] = None, early_stopping_rounds: Optional[int] = None, verbose: Optional[bool] = False, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Optional[List[Any]] = None, base_margin_eval_set: Optional[List[Any]] = None, feature_weights: Optional[Any] = None, callbacks: Optional[List[xgboost.callback.TrainingCallback]] = None) -> 'XGBRanker'\n",
      "     |      Fit gradient boosting ranker\n",
      "     |      \n",
      "     |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
      "     |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
      "     |      pass ``xgb_model`` argument.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X :\n",
      "     |          Feature matrix\n",
      "     |      y :\n",
      "     |          Labels\n",
      "     |      group :\n",
      "     |          Size of each query group of training data. Should have as many elements as the\n",
      "     |          query groups in the training data.  If this is set to None, then user must\n",
      "     |          provide qid.\n",
      "     |      qid :\n",
      "     |          Query ID for each training sample.  Should have the size of n_samples.  If\n",
      "     |          this is set to None, then user must provide group.\n",
      "     |      sample_weight :\n",
      "     |          Query group weights\n",
      "     |      \n",
      "     |          .. note:: Weights are per-group for ranking tasks\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each query group/id (not each\n",
      "     |              data point). This is because we only care about the relative ordering of\n",
      "     |              data points within each group, so it doesn't make sense to assign weights\n",
      "     |              to individual data points.\n",
      "     |      base_margin :\n",
      "     |          Global bias for each instance.\n",
      "     |      eval_set :\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      "     |          metrics will be computed.\n",
      "     |          Validation metrics will help us track the performance of the model.\n",
      "     |      eval_group :\n",
      "     |          A list in which ``eval_group[i]`` is the list containing the sizes of all\n",
      "     |          query groups in the ``i``-th pair in **eval_set**.\n",
      "     |      eval_qid :\n",
      "     |          A list in which ``eval_qid[i]`` is the array containing query ID of ``i``-th\n",
      "     |          pair in **eval_set**.\n",
      "     |      eval_metric :\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst.\n",
      "     |          If a list of str, should be the list of multiple built-in evaluation metrics\n",
      "     |          to use. The custom evaluation metric is not yet supported for the ranker.\n",
      "     |      early_stopping_rounds :\n",
      "     |          Activates early stopping. Validation metric needs to improve at least once in\n",
      "     |          every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
      "     |          least one item in **eval_set**.\n",
      "     |          The method returns the model from the last iteration (not the best one).  If\n",
      "     |          there's more than one item in **eval_set**, the last entry will be used for\n",
      "     |          early stopping.\n",
      "     |          If there's more than one metric in **eval_metric**, the last metric will be\n",
      "     |          used for early stopping.\n",
      "     |          If early stopping occurs, the model will have three additional fields:\n",
      "     |          ``clf.best_score``, ``clf.best_iteration`` and ``clf.best_ntree_limit``.\n",
      "     |      verbose :\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation metric\n",
      "     |          measured on the validation set to stderr.\n",
      "     |      xgb_model :\n",
      "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      sample_weight_eval_set :\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          group weights on the i-th validation set.\n",
      "     |      \n",
      "     |          .. note:: Weights are per-group for ranking tasks\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each query group (not each\n",
      "     |              data point). This is because we only care about the relative ordering of\n",
      "     |              data points within each group, so it doesn't make sense to assign\n",
      "     |              weights to individual data points.\n",
      "     |      base_margin_eval_set :\n",
      "     |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
      "     |          object storing base margin for the i-th validation set.\n",
      "     |      feature_weights :\n",
      "     |          Weight for each feature, defines the probability of each feature being\n",
      "     |          selected when colsample is being used.  All values must be greater than 0,\n",
      "     |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n",
      "     |          `exact` tree methods.\n",
      "     |      callbacks :\n",
      "     |          List of callback functions that are applied at end of each\n",
      "     |          iteration.  It is possible to use predefined callbacks by using\n",
      "     |          :ref:`callback_api`.  Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n",
      "     |                                                      save_best=True)]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __sklearn_is_fitted__(self) -> bool\n",
      "     |  \n",
      "     |  apply(self, X: Any, ntree_limit: int = 0, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      "     |      Return the predicted leaf every tree for each sample. If the model is trained with\n",
      "     |      early stopping, then `best_iteration` is used automatically.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      iteration_range :\n",
      "     |          See :py:meth:`xgboost.XGBRegressor.predict`.\n",
      "     |      \n",
      "     |      ntree_limit :\n",
      "     |          Deprecated, use ``iteration_range`` instead.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self) -> Dict[str, Dict[str, Union[List[float], List[Tuple[float, float]]]]]\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  get_booster(self) -> xgboost.core.Booster\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self) -> int\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self) -> Dict[str, Any]\n",
      "     |      Get xgboost specific parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n",
      "     |      Load the model from a file or bytearray. Path to file can be local\n",
      "     |      or as an URI.\n",
      "     |      \n",
      "     |      The model is loaded from XGBoost format which is universal among the various\n",
      "     |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n",
      "     |      feature_names) will not be loaded when using binary format.  To save those\n",
      "     |      attributes, use JSON instead.  See: `Model IO\n",
      "     |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      "     |      info.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname :\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  predict(self, X: Any, output_margin: bool = False, ntree_limit: Optional[int] = None, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      "     |      Predict with `X`.  If the model is trained with early stopping, then `best_iteration`\n",
      "     |      is used automatically.  For tree models, when data is on GPU, like cupy array or\n",
      "     |      cuDF dataframe and `predictor` is not specified, the prediction is run on GPU\n",
      "     |      automatically, otherwise it will run on CPU.\n",
      "     |      \n",
      "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X :\n",
      "     |          Data to predict with.\n",
      "     |      output_margin :\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit :\n",
      "     |          Deprecated, use `iteration_range` instead.\n",
      "     |      validate_features :\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are\n",
      "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      base_margin :\n",
      "     |          Margin added to prediction.\n",
      "     |      iteration_range :\n",
      "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      "     |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
      "     |          20)``, then only the forests built during [10, 20) (half open set) rounds are\n",
      "     |          used in this prediction.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction\n",
      "     |  \n",
      "     |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal format which is universal among the\n",
      "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
      "     |      (such as feature_names) will not be saved when using binary format.  To save those\n",
      "     |      attributes, use JSON instead. See: `Model IO\n",
      "     |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      "     |      info.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or os.PathLike\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params: Any) -> 'XGBModel'\n",
      "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
      "     |      parameters that are not defined as member variables in sklearn grid\n",
      "     |      search.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from XGBModel:\n",
      "     |  \n",
      "     |  best_iteration\n",
      "     |  \n",
      "     |  best_ntree_limit\n",
      "     |  \n",
      "     |  best_score\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as\n",
      "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
      "     |          learner types, such as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property, return depends on `importance_type` parameter.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
      "     |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class XGBRegressor(XGBModel, sklearn.base.RegressorMixin)\n",
      "     |  XGBRegressor(*, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'reg:squarederror', **kwargs: Any) -> None\n",
      "     |  \n",
      "     |  Implementation of the scikit-learn API for XGBoost regression.\n",
      "     |  \n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |      n_estimators : int\n",
      "     |          Number of gradient boosted trees.  Equivalent to number of boosting\n",
      "     |          rounds.\n",
      "     |  \n",
      "     |      max_depth :  Optional[int]\n",
      "     |          Maximum tree depth for base learners.\n",
      "     |      learning_rate : Optional[float]\n",
      "     |          Boosting learning rate (xgb's \"eta\")\n",
      "     |      verbosity : Optional[int]\n",
      "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |      objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      "     |          Specify the learning task and the corresponding learning objective or\n",
      "     |          a custom objective function to be used (see note below).\n",
      "     |      booster: Optional[str]\n",
      "     |          Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |      tree_method: Optional[str]\n",
      "     |          Specify which tree method to use.  Default to auto.  If this parameter\n",
      "     |          is set to default, XGBoost will choose the most conservative option\n",
      "     |          available.  It's recommended to study this option from the parameters\n",
      "     |          document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n",
      "     |      n_jobs : Optional[int]\n",
      "     |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "     |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "     |          balance the threads.  Creating thread contention will significantly slow down both\n",
      "     |          algorithms.\n",
      "     |      gamma : Optional[float]\n",
      "     |          Minimum loss reduction required to make a further partition on a leaf\n",
      "     |          node of the tree.\n",
      "     |      min_child_weight : Optional[float]\n",
      "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |      max_delta_step : Optional[float]\n",
      "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |      subsample : Optional[float]\n",
      "     |          Subsample ratio of the training instance.\n",
      "     |      colsample_bytree : Optional[float]\n",
      "     |          Subsample ratio of columns when constructing each tree.\n",
      "     |      colsample_bylevel : Optional[float]\n",
      "     |          Subsample ratio of columns for each level.\n",
      "     |      colsample_bynode : Optional[float]\n",
      "     |          Subsample ratio of columns for each split.\n",
      "     |      reg_alpha : Optional[float]\n",
      "     |          L1 regularization term on weights (xgb's alpha).\n",
      "     |      reg_lambda : Optional[float]\n",
      "     |          L2 regularization term on weights (xgb's lambda).\n",
      "     |      scale_pos_weight : Optional[float]\n",
      "     |          Balancing of positive and negative weights.\n",
      "     |      base_score : Optional[float]\n",
      "     |          The initial prediction score of all instances, global bias.\n",
      "     |      random_state : Optional[Union[numpy.random.RandomState, int]]\n",
      "     |          Random number seed.\n",
      "     |  \n",
      "     |          .. note::\n",
      "     |  \n",
      "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      "     |             it uses Hogwild algorithm.\n",
      "     |  \n",
      "     |      missing : float, default np.nan\n",
      "     |          Value in the data which needs to be present as a missing value.\n",
      "     |      num_parallel_tree: Optional[int]\n",
      "     |          Used for boosting random forest.\n",
      "     |      monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
      "     |          Constraint of variable monotonicity.  See tutorial for more\n",
      "     |          information.\n",
      "     |      interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
      "     |          Constraints for interaction representing permitted interactions.  The\n",
      "     |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "     |          [2, 3, 4]], where each inner list is a group of indices of features\n",
      "     |          that are allowed to interact with each other.  See tutorial for more\n",
      "     |          information\n",
      "     |      importance_type: Optional[str]\n",
      "     |          The feature importance type for the feature_importances\\_ property:\n",
      "     |  \n",
      "     |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      "     |            \"total_cover\".\n",
      "     |          * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
      "     |            without bias.\n",
      "     |  \n",
      "     |      gpu_id : Optional[int]\n",
      "     |          Device ordinal.\n",
      "     |      validate_parameters : Optional[bool]\n",
      "     |          Give warnings for unknown parameter.\n",
      "     |      predictor : Optional[str]\n",
      "     |          Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n",
      "     |          gpu_predictor].\n",
      "     |      enable_categorical : bool\n",
      "     |  \n",
      "     |          .. versionadded:: 1.5.0\n",
      "     |  \n",
      "     |          Experimental support for categorical data.  Do not set to true unless you are\n",
      "     |          interested in development. Only valid when `gpu_hist` and dataframe are used.\n",
      "     |  \n",
      "     |      kwargs : dict, optional\n",
      "     |          Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "     |          parameters can be found here:\n",
      "     |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "     |          dict simultaneously will result in a TypeError.\n",
      "     |  \n",
      "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "     |              that parameters passed via this argument will interact properly\n",
      "     |              with scikit-learn.\n",
      "     |  \n",
      "     |          .. note::  Custom objective function\n",
      "     |  \n",
      "     |              A custom objective function can be provided for the ``objective``\n",
      "     |              parameter. In this case, it should have the signature\n",
      "     |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |              y_true: array_like of shape [n_samples]\n",
      "     |                  The target values\n",
      "     |              y_pred: array_like of shape [n_samples]\n",
      "     |                  The predicted values\n",
      "     |  \n",
      "     |              grad: array_like of shape [n_samples]\n",
      "     |                  The value of the gradient for each sample point.\n",
      "     |              hess: array_like of shape [n_samples]\n",
      "     |                  The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRegressor\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'reg:squarederror', **kwargs: Any) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __sklearn_is_fitted__(self) -> bool\n",
      "     |  \n",
      "     |  apply(self, X: Any, ntree_limit: int = 0, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      "     |      Return the predicted leaf every tree for each sample. If the model is trained with\n",
      "     |      early stopping, then `best_iteration` is used automatically.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      iteration_range :\n",
      "     |          See :py:meth:`xgboost.XGBRegressor.predict`.\n",
      "     |      \n",
      "     |      ntree_limit :\n",
      "     |          Deprecated, use ``iteration_range`` instead.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self) -> Dict[str, Dict[str, Union[List[float], List[Tuple[float, float]]]]]\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[List[Tuple[Any, Any]]] = None, eval_metric: Union[str, List[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]], NoneType] = None, early_stopping_rounds: Optional[int] = None, verbose: Optional[bool] = True, xgb_model: Union[xgboost.core.Booster, ForwardRef('XGBModel'), str, NoneType] = None, sample_weight_eval_set: Optional[List[Any]] = None, base_margin_eval_set: Optional[List[Any]] = None, feature_weights: Optional[Any] = None, callbacks: Optional[List[xgboost.callback.TrainingCallback]] = None) -> 'XGBModel'\n",
      "     |      Fit gradient boosting model.\n",
      "     |      \n",
      "     |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
      "     |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
      "     |      pass ``xgb_model`` argument.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X :\n",
      "     |          Feature matrix\n",
      "     |      y :\n",
      "     |          Labels\n",
      "     |      sample_weight :\n",
      "     |          instance weights\n",
      "     |      base_margin :\n",
      "     |          global bias for each instance.\n",
      "     |      eval_set :\n",
      "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      "     |          metrics will be computed.\n",
      "     |          Validation metrics will help us track the performance of the model.\n",
      "     |      eval_metric :\n",
      "     |          If a str, should be a built-in evaluation metric to use. See doc/parameter.rst.\n",
      "     |      \n",
      "     |          If a list of str, should be the list of multiple built-in evaluation metrics\n",
      "     |          to use.\n",
      "     |      \n",
      "     |          If callable, a custom evaluation metric. The call signature is\n",
      "     |          ``func(y_predicted, y_true)`` where ``y_true`` will be a DMatrix object such\n",
      "     |          that you may need to call the ``get_label`` method. It must return a str,\n",
      "     |          value pair where the str is a name for the evaluation and value is the value\n",
      "     |          of the evaluation function. The callable custom objective is always minimized.\n",
      "     |      early_stopping_rounds :\n",
      "     |          Activates early stopping. Validation metric needs to improve at least once in\n",
      "     |          every **early_stopping_rounds** round(s) to continue training.\n",
      "     |          Requires at least one item in **eval_set**.\n",
      "     |      \n",
      "     |          The method returns the model from the last iteration (not the best one).\n",
      "     |          If there's more than one item in **eval_set**, the last entry will be used\n",
      "     |          for early stopping.\n",
      "     |      \n",
      "     |          If there's more than one metric in **eval_metric**, the last metric will be\n",
      "     |          used for early stopping.\n",
      "     |      \n",
      "     |          If early stopping occurs, the model will have three additional fields:\n",
      "     |          ``clf.best_score``, ``clf.best_iteration``.\n",
      "     |      verbose :\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation metric\n",
      "     |          measured on the validation set to stderr.\n",
      "     |      xgb_model :\n",
      "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      sample_weight_eval_set :\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
      "     |          object storing instance weights for the i-th validation set.\n",
      "     |      base_margin_eval_set :\n",
      "     |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
      "     |          object storing base margin for the i-th validation set.\n",
      "     |      feature_weights :\n",
      "     |          Weight for each feature, defines the probability of each feature being\n",
      "     |          selected when colsample is being used.  All values must be greater than 0,\n",
      "     |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n",
      "     |          `exact` tree methods.\n",
      "     |      callbacks :\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n",
      "     |                                                      save_best=True)]\n",
      "     |  \n",
      "     |  get_booster(self) -> xgboost.core.Booster\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self) -> int\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self) -> Dict[str, Any]\n",
      "     |      Get xgboost specific parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n",
      "     |      Load the model from a file or bytearray. Path to file can be local\n",
      "     |      or as an URI.\n",
      "     |      \n",
      "     |      The model is loaded from XGBoost format which is universal among the various\n",
      "     |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n",
      "     |      feature_names) will not be loaded when using binary format.  To save those\n",
      "     |      attributes, use JSON instead.  See: `Model IO\n",
      "     |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      "     |      info.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname :\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  predict(self, X: Any, output_margin: bool = False, ntree_limit: Optional[int] = None, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      "     |      Predict with `X`.  If the model is trained with early stopping, then `best_iteration`\n",
      "     |      is used automatically.  For tree models, when data is on GPU, like cupy array or\n",
      "     |      cuDF dataframe and `predictor` is not specified, the prediction is run on GPU\n",
      "     |      automatically, otherwise it will run on CPU.\n",
      "     |      \n",
      "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X :\n",
      "     |          Data to predict with.\n",
      "     |      output_margin :\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit :\n",
      "     |          Deprecated, use `iteration_range` instead.\n",
      "     |      validate_features :\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are\n",
      "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      base_margin :\n",
      "     |          Margin added to prediction.\n",
      "     |      iteration_range :\n",
      "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      "     |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
      "     |          20)``, then only the forests built during [10, 20) (half open set) rounds are\n",
      "     |          used in this prediction.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4.0\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction\n",
      "     |  \n",
      "     |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal format which is universal among the\n",
      "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
      "     |      (such as feature_names) will not be saved when using binary format.  To save those\n",
      "     |      attributes, use JSON instead. See: `Model IO\n",
      "     |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n",
      "     |      info.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or os.PathLike\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params: Any) -> 'XGBModel'\n",
      "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
      "     |      parameters that are not defined as member variables in sklearn grid\n",
      "     |      search.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from XGBModel:\n",
      "     |  \n",
      "     |  best_iteration\n",
      "     |  \n",
      "     |  best_ntree_limit\n",
      "     |  \n",
      "     |  best_score\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as\n",
      "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
      "     |          learner types, such as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property, return depends on `importance_type` parameter.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
      "     |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "\n",
      "FUNCTIONS\n",
      "    config_context(**new_config)\n",
      "        Context manager for global XGBoost configuration.\n",
      "        \n",
      "        \n",
      "        Global configuration consists of a collection of parameters that can be applied in the\n",
      "        global scope. See https://xgboost.readthedocs.io/en/stable/parameter.html for the full\n",
      "        list of parameters supported in the global configuration.\n",
      "        \n",
      "        \n",
      "        .. note::\n",
      "        \n",
      "            All settings, not just those presently modified, will be returned to their\n",
      "            previous values when the context manager is exited. This is not thread-safe.\n",
      "                \n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        new_config: Dict[str, Any]\n",
      "            Keyword arguments representing the parameters and their values\n",
      "                \n",
      "        Example\n",
      "        -------\n",
      "        \n",
      "        .. code-block:: python\n",
      "        \n",
      "            import xgboost as xgb\n",
      "        \n",
      "            # Show all messages, including ones pertaining to debugging\n",
      "            xgb.set_config(verbosity=2)\n",
      "        \n",
      "            # Get current value of global configuration\n",
      "            # This is a dict containing all parameters in the global configuration,\n",
      "            # including 'verbosity'\n",
      "            config = xgb.get_config()\n",
      "            assert config['verbosity'] == 2\n",
      "        \n",
      "            # Example of using the context manager xgb.config_context().\n",
      "            # The context manager will restore the previous value of the global\n",
      "            # configuration upon exiting.\n",
      "            with xgb.config_context(verbosity=0):\n",
      "                # Suppress warning caused by model generated with XGBoost version < 1.0.0\n",
      "                bst = xgb.Booster(model_file='./old_model.bin')\n",
      "            assert xgb.get_config()['verbosity'] == 2  # old value restored\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        set_config: Set global XGBoost configuration\n",
      "        get_config: Get current values of the global configuration\n",
      "    \n",
      "    cv(params, dtrain, num_boost_round=10, nfold=3, stratified=False, folds=None, metrics=(), obj=None, feval=None, maximize=None, early_stopping_rounds=None, fpreproc=None, as_pandas=True, verbose_eval=None, show_stdv=True, seed=0, callbacks=None, shuffle=True)\n",
      "        Cross-validation with given parameters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        params : dict\n",
      "            Booster params.\n",
      "        dtrain : DMatrix\n",
      "            Data to be trained.\n",
      "        num_boost_round : int\n",
      "            Number of boosting iterations.\n",
      "        nfold : int\n",
      "            Number of folds in CV.\n",
      "        stratified : bool\n",
      "            Perform stratified sampling.\n",
      "        folds : a KFold or StratifiedKFold instance or list of fold indices\n",
      "            Sklearn KFolds or StratifiedKFolds object.\n",
      "            Alternatively may explicitly pass sample indices for each fold.\n",
      "            For ``n`` folds, **folds** should be a length ``n`` list of tuples.\n",
      "            Each tuple is ``(in,out)`` where ``in`` is a list of indices to be used\n",
      "            as the training samples for the ``n`` th fold and ``out`` is a list of\n",
      "            indices to be used as the testing samples for the ``n`` th fold.\n",
      "        metrics : string or list of strings\n",
      "            Evaluation metrics to be watched in CV.\n",
      "        obj : function\n",
      "            Custom objective function.\n",
      "        feval : function\n",
      "            Custom evaluation function.\n",
      "        maximize : bool\n",
      "            Whether to maximize feval.\n",
      "        early_stopping_rounds: int\n",
      "            Activates early stopping. Cross-Validation metric (average of validation\n",
      "            metric computed over CV folds) needs to improve at least once in\n",
      "            every **early_stopping_rounds** round(s) to continue training.\n",
      "            The last entry in the evaluation history will represent the best iteration.\n",
      "            If there's more than one metric in the **eval_metric** parameter given in\n",
      "            **params**, the last metric will be used for early stopping.\n",
      "        fpreproc : function\n",
      "            Preprocessing function that takes (dtrain, dtest, param) and returns\n",
      "            transformed versions of those.\n",
      "        as_pandas : bool, default True\n",
      "            Return pd.DataFrame when pandas is installed.\n",
      "            If False or pandas is not installed, return np.ndarray\n",
      "        verbose_eval : bool, int, or None, default None\n",
      "            Whether to display the progress. If None, progress will be displayed\n",
      "            when np.ndarray is returned. If True, progress will be displayed at\n",
      "            boosting stage. If an integer is given, progress will be displayed\n",
      "            at every given `verbose_eval` boosting stage.\n",
      "        show_stdv : bool, default True\n",
      "            Whether to display the standard deviation in progress.\n",
      "            Results are not affected, and always contains std.\n",
      "        seed : int\n",
      "            Seed used to generate the folds (passed to numpy.random.seed).\n",
      "        callbacks : list of callback functions\n",
      "            List of callback functions that are applied at end of each iteration.\n",
      "            It is possible to use predefined callbacks by using\n",
      "            :ref:`Callback API <callback_api>`.\n",
      "            Example:\n",
      "        \n",
      "            .. code-block:: python\n",
      "        \n",
      "                [xgb.callback.LearningRateScheduler(custom_rates)]\n",
      "        shuffle : bool\n",
      "            Shuffle data before creating folds.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        evaluation history : list(string)\n",
      "    \n",
      "    get_config()\n",
      "        Get current values of the global configuration.\n",
      "        \n",
      "        \n",
      "        Global configuration consists of a collection of parameters that can be applied in the\n",
      "        global scope. See https://xgboost.readthedocs.io/en/stable/parameter.html for the full\n",
      "        list of parameters supported in the global configuration.\n",
      "        \n",
      "        \n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        args: Dict[str, Any]\n",
      "            The list of global parameters and their values\n",
      "                \n",
      "        Example\n",
      "        -------\n",
      "        \n",
      "        .. code-block:: python\n",
      "        \n",
      "            import xgboost as xgb\n",
      "        \n",
      "            # Show all messages, including ones pertaining to debugging\n",
      "            xgb.set_config(verbosity=2)\n",
      "        \n",
      "            # Get current value of global configuration\n",
      "            # This is a dict containing all parameters in the global configuration,\n",
      "            # including 'verbosity'\n",
      "            config = xgb.get_config()\n",
      "            assert config['verbosity'] == 2\n",
      "        \n",
      "            # Example of using the context manager xgb.config_context().\n",
      "            # The context manager will restore the previous value of the global\n",
      "            # configuration upon exiting.\n",
      "            with xgb.config_context(verbosity=0):\n",
      "                # Suppress warning caused by model generated with XGBoost version < 1.0.0\n",
      "                bst = xgb.Booster(model_file='./old_model.bin')\n",
      "            assert xgb.get_config()['verbosity'] == 2  # old value restored\n",
      "    \n",
      "    plot_importance(booster, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance', xlabel='F score', ylabel='Features', fmap='', importance_type='weight', max_num_features=None, grid=True, show_values=True, **kwargs)\n",
      "        Plot importance based on fitted trees.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster, XGBModel or dict\n",
      "            Booster or XGBModel instance, or dict taken by Booster.get_fscore()\n",
      "        ax : matplotlib Axes, default None\n",
      "            Target axes instance. If None, new figure and axes will be created.\n",
      "        grid : bool, Turn the axes grids on or off.  Default is True (On).\n",
      "        importance_type : str, default \"weight\"\n",
      "            How the importance is calculated: either \"weight\", \"gain\", or \"cover\"\n",
      "        \n",
      "            * \"weight\" is the number of times a feature appears in a tree\n",
      "            * \"gain\" is the average gain of splits which use the feature\n",
      "            * \"cover\" is the average coverage of splits which use the feature\n",
      "              where coverage is defined as the number of samples affected by the split\n",
      "        max_num_features : int, default None\n",
      "            Maximum number of top features displayed on plot. If None, all features will be displayed.\n",
      "        height : float, default 0.2\n",
      "            Bar height, passed to ax.barh()\n",
      "        xlim : tuple, default None\n",
      "            Tuple passed to axes.xlim()\n",
      "        ylim : tuple, default None\n",
      "            Tuple passed to axes.ylim()\n",
      "        title : str, default \"Feature importance\"\n",
      "            Axes title. To disable, pass None.\n",
      "        xlabel : str, default \"F score\"\n",
      "            X axis title label. To disable, pass None.\n",
      "        ylabel : str, default \"Features\"\n",
      "            Y axis title label. To disable, pass None.\n",
      "        fmap: str or os.PathLike (optional)\n",
      "            The name of feature map file.\n",
      "        show_values : bool, default True\n",
      "            Show values on plot. To disable, pass False.\n",
      "        kwargs :\n",
      "            Other keywords passed to ax.barh()\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib Axes\n",
      "    \n",
      "    plot_tree(booster, fmap='', num_trees=0, rankdir=None, ax=None, **kwargs)\n",
      "        Plot specified tree.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster, XGBModel\n",
      "            Booster or XGBModel instance\n",
      "        fmap: str (optional)\n",
      "           The name of feature map file\n",
      "        num_trees : int, default 0\n",
      "            Specify the ordinal number of target tree\n",
      "        rankdir : str, default \"TB\"\n",
      "            Passed to graphiz via graph_attr\n",
      "        ax : matplotlib Axes, default None\n",
      "            Target axes instance. If None, new figure and axes will be created.\n",
      "        kwargs :\n",
      "            Other keywords passed to to_graphviz\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib Axes\n",
      "    \n",
      "    set_config(**new_config)\n",
      "        Set global configuration.\n",
      "        \n",
      "        \n",
      "        Global configuration consists of a collection of parameters that can be applied in the\n",
      "        global scope. See https://xgboost.readthedocs.io/en/stable/parameter.html for the full\n",
      "        list of parameters supported in the global configuration.\n",
      "        \n",
      "        \n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        new_config: Dict[str, Any]\n",
      "            Keyword arguments representing the parameters and their values\n",
      "                \n",
      "        Example\n",
      "        -------\n",
      "        \n",
      "        .. code-block:: python\n",
      "        \n",
      "            import xgboost as xgb\n",
      "        \n",
      "            # Show all messages, including ones pertaining to debugging\n",
      "            xgb.set_config(verbosity=2)\n",
      "        \n",
      "            # Get current value of global configuration\n",
      "            # This is a dict containing all parameters in the global configuration,\n",
      "            # including 'verbosity'\n",
      "            config = xgb.get_config()\n",
      "            assert config['verbosity'] == 2\n",
      "        \n",
      "            # Example of using the context manager xgb.config_context().\n",
      "            # The context manager will restore the previous value of the global\n",
      "            # configuration upon exiting.\n",
      "            with xgb.config_context(verbosity=0):\n",
      "                # Suppress warning caused by model generated with XGBoost version < 1.0.0\n",
      "                bst = xgb.Booster(model_file='./old_model.bin')\n",
      "            assert xgb.get_config()['verbosity'] == 2  # old value restored\n",
      "    \n",
      "    to_graphviz(booster, fmap='', num_trees=0, rankdir=None, yes_color=None, no_color=None, condition_node_params=None, leaf_node_params=None, **kwargs)\n",
      "        Convert specified tree to graphviz instance. IPython can automatically plot\n",
      "        the returned graphiz instance. Otherwise, you should call .render() method\n",
      "        of the returned graphiz instance.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster, XGBModel\n",
      "            Booster or XGBModel instance\n",
      "        fmap: str (optional)\n",
      "           The name of feature map file\n",
      "        num_trees : int, default 0\n",
      "            Specify the ordinal number of target tree\n",
      "        rankdir : str, default \"UT\"\n",
      "            Passed to graphiz via graph_attr\n",
      "        yes_color : str, default '#0000FF'\n",
      "            Edge color when meets the node condition.\n",
      "        no_color : str, default '#FF0000'\n",
      "            Edge color when doesn't meet the node condition.\n",
      "        condition_node_params : dict, optional\n",
      "            Condition node configuration for for graphviz.  Example:\n",
      "        \n",
      "            .. code-block:: python\n",
      "        \n",
      "                {'shape': 'box',\n",
      "                 'style': 'filled,rounded',\n",
      "                 'fillcolor': '#78bceb'}\n",
      "        \n",
      "        leaf_node_params : dict, optional\n",
      "            Leaf node configuration for graphviz. Example:\n",
      "        \n",
      "            .. code-block:: python\n",
      "        \n",
      "                {'shape': 'box',\n",
      "                 'style': 'filled',\n",
      "                 'fillcolor': '#e48038'}\n",
      "        \n",
      "        \\*\\*kwargs: dict, optional\n",
      "            Other keywords passed to graphviz graph_attr, e.g. ``graph [ {key} = {value} ]``\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        graph: graphviz.Source\n",
      "    \n",
      "    train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=None, early_stopping_rounds=None, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None)\n",
      "        Train a booster with given parameters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        params : dict\n",
      "            Booster params.\n",
      "        dtrain : DMatrix\n",
      "            Data to be trained.\n",
      "        num_boost_round: int\n",
      "            Number of boosting iterations.\n",
      "        evals: list of pairs (DMatrix, string)\n",
      "            List of validation sets for which metrics will evaluated during training.\n",
      "            Validation metrics will help us track the performance of the model.\n",
      "        obj : function\n",
      "            Customized objective function.\n",
      "        feval : function\n",
      "            Customized evaluation function.\n",
      "        maximize : bool\n",
      "            Whether to maximize feval.\n",
      "        early_stopping_rounds: int\n",
      "            Activates early stopping. Validation metric needs to improve at least once in\n",
      "            every **early_stopping_rounds** round(s) to continue training.\n",
      "            Requires at least one item in **evals**.\n",
      "            The method returns the model from the last iteration (not the best one).  Use\n",
      "            custom callback or model slicing if the best model is desired.\n",
      "            If there's more than one item in **evals**, the last entry will be used for early\n",
      "            stopping.\n",
      "            If there's more than one metric in the **eval_metric** parameter given in\n",
      "            **params**, the last metric will be used for early stopping.\n",
      "            If early stopping occurs, the model will have three additional fields:\n",
      "            ``bst.best_score``, ``bst.best_iteration``.\n",
      "        evals_result: dict\n",
      "            This dictionary stores the evaluation results of all the items in watchlist.\n",
      "        \n",
      "            Example: with a watchlist containing\n",
      "            ``[(dtest,'eval'), (dtrain,'train')]`` and\n",
      "            a parameter containing ``('eval_metric': 'logloss')``,\n",
      "            the **evals_result** returns\n",
      "        \n",
      "            .. code-block:: python\n",
      "        \n",
      "                {'train': {'logloss': ['0.48253', '0.35953']},\n",
      "                 'eval': {'logloss': ['0.480385', '0.357756']}}\n",
      "        \n",
      "        verbose_eval : bool or int\n",
      "            Requires at least one item in **evals**.\n",
      "            If **verbose_eval** is True then the evaluation metric on the validation set is\n",
      "            printed at each boosting stage.\n",
      "            If **verbose_eval** is an integer then the evaluation metric on the validation set\n",
      "            is printed at every given **verbose_eval** boosting stage. The last boosting stage\n",
      "            / the boosting stage found by using **early_stopping_rounds** is also printed.\n",
      "            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric\n",
      "            is printed every 4 boosting stages, instead of every boosting stage.\n",
      "        xgb_model : file name of stored xgb model or 'Booster' instance\n",
      "            Xgb model to be loaded before training (allows training continuation).\n",
      "        callbacks : list of callback functions\n",
      "            List of callback functions that are applied at end of each iteration.\n",
      "            It is possible to use predefined callbacks by using\n",
      "            :ref:`Callback API <callback_api>`.\n",
      "            Example:\n",
      "        \n",
      "            .. code-block:: python\n",
      "        \n",
      "                [xgb.callback.LearningRateScheduler(custom_rates)]\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Booster : a trained booster model\n",
      "\n",
      "DATA\n",
      "    __all__ = ['DMatrix', 'DeviceQuantileDMatrix', 'Booster', 'DataIter', ...\n",
      "\n",
      "VERSION\n",
      "    1.5.2\n",
      "\n",
      "FILE\n",
      "    c:\\users\\manuelsousa\\anaconda3\\lib\\site-packages\\xgboost\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "017bb47b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:57:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ManuelSousa\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  1.0\n",
      "Testing Accuracy:  0.872\n"
     ]
    }
   ],
   "source": [
    "# Utilizando o método XGBClassifier\n",
    "clf = XGBClassifier(learning_rate=0.5, n_jobs=-1, n_estimators=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred_train = clf.predict(X_train)\n",
    "acc_train = accuracy_score(y_train, pred_train)\n",
    "print(\"Training Accuracy: \", acc_train)\n",
    "\n",
    "pred_test = clf.predict(X_test)\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "print(\"Testing Accuracy: \", acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "4f760a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc_result = pd.DataFrame({'Model': ['XGB Classifier'], 'Train Accuracy': [acc_train], 'Test Accuracy': [acc_test]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "8a0cde5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r3 = r2.append(xgbc_result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "03516ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.869739</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging Decision Tree</td>\n",
       "      <td>0.881764</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest Tree</td>\n",
       "      <td>0.915832</td>\n",
       "      <td>0.848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGB Classifier</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Train Accuracy  Test Accuracy\n",
       "0          Decision Tree        0.869739          0.752\n",
       "1  Bagging Decision Tree        0.881764          0.800\n",
       "2     Random Forest Tree        0.915832          0.848\n",
       "3         XGB Classifier        1.000000          0.872"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "250bcdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter as labels para numérico\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_train_enc = label_encoder.fit_transform(y_train)\n",
    "y_train_enc = pd.DataFrame(y_train_enc)\n",
    "\n",
    "y_test_enc = label_encoder.fit_transform(y_test)\n",
    "y_test_enc = pd.DataFrame(y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "6c350451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0    2\n",
       "1    1\n",
       "2    2\n",
       "3    1\n",
       "4    1\n",
       "..  ..\n",
       "494  0\n",
       "495  2\n",
       "496  0\n",
       "497  0\n",
       "498  2\n",
       "\n",
       "[499 rows x 1 columns]"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "075461bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:59:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_rounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.18236\ttest-merror:0.28000\n",
      "[1]\ttrain-merror:0.13627\ttest-merror:0.23200\n",
      "[2]\ttrain-merror:0.12826\ttest-merror:0.21600\n",
      "[3]\ttrain-merror:0.11022\ttest-merror:0.20000\n",
      "[4]\ttrain-merror:0.10220\ttest-merror:0.19200\n",
      "[5]\ttrain-merror:0.09419\ttest-merror:0.17600\n",
      "[6]\ttrain-merror:0.09018\ttest-merror:0.16000\n",
      "[7]\ttrain-merror:0.09018\ttest-merror:0.16800\n",
      "[8]\ttrain-merror:0.08617\ttest-merror:0.16000\n",
      "[9]\ttrain-merror:0.07816\ttest-merror:0.14400\n",
      "[10]\ttrain-merror:0.07214\ttest-merror:0.13600\n",
      "[11]\ttrain-merror:0.06413\ttest-merror:0.13600\n",
      "[12]\ttrain-merror:0.06413\ttest-merror:0.13600\n",
      "[13]\ttrain-merror:0.05411\ttest-merror:0.13600\n",
      "[14]\ttrain-merror:0.04810\ttest-merror:0.13600\n",
      "[15]\ttrain-merror:0.04810\ttest-merror:0.13600\n",
      "[16]\ttrain-merror:0.04008\ttest-merror:0.13600\n",
      "[17]\ttrain-merror:0.03206\ttest-merror:0.13600\n",
      "[18]\ttrain-merror:0.03006\ttest-merror:0.12000\n",
      "[19]\ttrain-merror:0.02605\ttest-merror:0.12000\n",
      "[20]\ttrain-merror:0.02605\ttest-merror:0.12800\n",
      "[21]\ttrain-merror:0.02605\ttest-merror:0.12800\n",
      "[22]\ttrain-merror:0.02605\ttest-merror:0.12000\n",
      "[23]\ttrain-merror:0.02204\ttest-merror:0.11200\n",
      "[24]\ttrain-merror:0.02204\ttest-merror:0.12000\n",
      "[25]\ttrain-merror:0.02004\ttest-merror:0.12800\n",
      "[26]\ttrain-merror:0.01603\ttest-merror:0.12000\n",
      "[27]\ttrain-merror:0.01202\ttest-merror:0.12000\n",
      "[28]\ttrain-merror:0.01002\ttest-merror:0.11200\n",
      "[29]\ttrain-merror:0.00802\ttest-merror:0.14400\n",
      "[30]\ttrain-merror:0.00802\ttest-merror:0.14400\n",
      "[31]\ttrain-merror:0.00601\ttest-merror:0.13600\n",
      "[32]\ttrain-merror:0.00601\ttest-merror:0.13600\n",
      "[33]\ttrain-merror:0.00601\ttest-merror:0.13600\n",
      "[34]\ttrain-merror:0.00601\ttest-merror:0.13600\n",
      "[35]\ttrain-merror:0.00601\ttest-merror:0.13600\n",
      "[36]\ttrain-merror:0.00401\ttest-merror:0.12800\n",
      "[37]\ttrain-merror:0.00401\ttest-merror:0.13600\n",
      "[38]\ttrain-merror:0.00401\ttest-merror:0.13600\n",
      "[39]\ttrain-merror:0.00401\ttest-merror:0.12800\n",
      "[40]\ttrain-merror:0.00200\ttest-merror:0.14400\n",
      "[41]\ttrain-merror:0.00200\ttest-merror:0.13600\n",
      "[42]\ttrain-merror:0.00000\ttest-merror:0.13600\n",
      "[43]\ttrain-merror:0.00000\ttest-merror:0.13600\n",
      "[44]\ttrain-merror:0.00000\ttest-merror:0.13600\n",
      "[45]\ttrain-merror:0.00000\ttest-merror:0.13600\n",
      "[46]\ttrain-merror:0.00000\ttest-merror:0.13600\n",
      "[47]\ttrain-merror:0.00000\ttest-merror:0.13600\n",
      "[48]\ttrain-merror:0.00000\ttest-merror:0.13600\n",
      "[49]\ttrain-merror:0.00000\ttest-merror:0.13600\n",
      "[50]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[51]\ttrain-merror:0.00000\ttest-merror:0.13600\n",
      "[52]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[53]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[54]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[55]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[56]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[57]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[58]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[59]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[60]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[61]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[62]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[63]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[64]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[65]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[66]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[67]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[68]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[69]\ttrain-merror:0.00000\ttest-merror:0.13600\n",
      "[70]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[71]\ttrain-merror:0.00000\ttest-merror:0.13600\n",
      "[72]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[73]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[74]\ttrain-merror:0.00000\ttest-merror:0.14400\n",
      "[75]\ttrain-merror:0.00000\ttest-merror:0.13600\n",
      "[76]\ttrain-merror:0.00000\ttest-merror:0.13600\n",
      "[77]\ttrain-merror:0.00000\ttest-merror:0.15200\n",
      "[78]\ttrain-merror:0.00000\ttest-merror:0.15200\n",
      "[79]\ttrain-merror:0.00000\ttest-merror:0.14400\n"
     ]
    }
   ],
   "source": [
    "# Utilizando outro método\n",
    "param = {'objective':'multi:softmax',\n",
    "     'eta':0.2,\n",
    "     'max_depth':15,\n",
    "     'num_class':54,\n",
    "     'eval_metric': 'merror',\n",
    "     'nthread':'32',\n",
    "     'early_stopping_rounds': 1}\n",
    "\n",
    "# XGBoost wants the data serialized into a data structure called Dmatrix\n",
    "dat = xgb.DMatrix(X_train, label=y_train_enc)\n",
    "datt = xgb.DMatrix(X_test, label=y_test_enc)\n",
    "watch = [(dat, 'train'), (datt, 'test')]\n",
    "score = {}\n",
    "\n",
    "mxgb = xgb.train(param, dat, 80, watch, evals_result=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "dd0d5e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "\n",
    "epoch = list(range(len(score['test']['merror'])))\n",
    "pred_y = mxgb.predict(datt)\n",
    "acc_test = accuracy_score(y_test_enc, pred_y)\n",
    "plt.plot(epoch, score['test']['merror'], label='test')\n",
    "plt.title(\"Final Error %0.4f\" % acc_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "498d32bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  1.0\n",
      "Testing Accuracy:  0.856\n"
     ]
    }
   ],
   "source": [
    "pred_y = mxgb.predict(dat)\n",
    "acc_train = accuracy_score(y_train_enc, pred_y)\n",
    "print(\"Training Accuracy: \", acc_train)\n",
    "print(\"Testing Accuracy: \", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "ae916b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_result = pd.DataFrame({'Model': ['XGB[multi:softmax]'], 'Train Accuracy': [acc_train], 'Test Accuracy': [acc_test]})\n",
    "r4 = r3.append(xgb_result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9df587",
   "metadata": {},
   "source": [
    "## Results Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "bcbe4322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.869739</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging Decision Tree</td>\n",
       "      <td>0.881764</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest Tree</td>\n",
       "      <td>0.915832</td>\n",
       "      <td>0.848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGB Classifier</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGB[multi:softmax]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Train Accuracy  Test Accuracy\n",
       "0          Decision Tree        0.869739          0.752\n",
       "1  Bagging Decision Tree        0.881764          0.800\n",
       "2     Random Forest Tree        0.915832          0.848\n",
       "3         XGB Classifier        1.000000          0.872\n",
       "4     XGB[multi:softmax]        1.000000          0.856"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c5e9d6",
   "metadata": {},
   "source": [
    "#### Random Forest and XGBoost based methods achieved the best model performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
